{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES DE ACTIVACIÓN PARA REDES NEURONALES MULTICAPA\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit,softmax\n",
    "\n",
    "class function(object):\n",
    "    def __init__(self,funcion,derivative=None,rand_init=(0,1)):\n",
    "        self.F=funcion\n",
    "        self.D=derivative\n",
    "        self.Rand_init=rand_init\n",
    "\n",
    "lineal=function(funcion=lambda x:x,\n",
    "                derivative=lambda x:1,\n",
    "                rand_init=(-1,1))\n",
    "\n",
    "sigm=function(funcion=lambda x: expit(x),\n",
    "              derivative=lambda x: expit(x)*(1-expit(x)),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "tanh=function(funcion=lambda x:(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)),\n",
    "              derivative=lambda x:1-((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))**2,\n",
    "              rand_init=(-1,1))\n",
    "\n",
    "tanh1=function(funcion=lambda x:np.tanh(x),\n",
    "               derivative=lambda x:1-np.tanh(x)**2,\n",
    "               rand_init=(-1,1))\n",
    "\n",
    "relu=function(funcion=lambda x: np.maximum(0, x),\n",
    "              derivative=lambda x: np.where(x<=0,0,1),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "softmaxf=function(funcion=lambda x: softmax(x),\n",
    "                  derivative=lambda x:softmax(x)*(1-softmax(x)),\n",
    "                  rand_init=(0,1))\n",
    "\n",
    "\n",
    "# funciones de coste\n",
    "mse=function(funcion=lambda Yp, Yr: np.mean((Yp - Yr) ** 2) ,\n",
    "                 derivative=lambda Yp, Yr: (Yp - Yr))\n",
    "\n",
    "cross_entropy=function(funcion=lambda yscore,yreal:-np.sum(yreal*np.log(yscore))/yscore.shape[0],\n",
    "                       derivative=lambda yscore,yreal:yscore-yreal)\n",
    "\n",
    "Funciones={\"relu\":relu,\n",
    "           \"sigm\":sigm,\n",
    "           \"relu\":relu,\n",
    "           \"tanh\":tanh,\n",
    "           \"tanh1\":tanh1,\n",
    "           \"lineal\":lineal,\n",
    "           \"softmax\":softmaxf}\n",
    "\n",
    "Loss={\"mse\":mse,\n",
    "      \"cross_entropy\":cross_entropy}\n",
    "\n",
    "_x = np.linspace(-10, 10, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "Min_Max = preprocessing.MinMaxScaler()\n",
    "Ordinal =preprocessing.OrdinalEncoder()\n",
    "\n",
    "\n",
    "def one_hot_cols(df,cols_to_one):\n",
    "    one_hot=pd.get_dummies(df,cols_to_one,columns=cols_to_one)\n",
    "    return one_hot\n",
    "\n",
    "def fit_cols(df, cols_to_fit,fit_function ):\n",
    "    for col in cols_to_fit:\n",
    "        df[col] = pd.DataFrame(fit_function.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
    "    return df\n",
    "\n",
    "def split_Dataset(mypandas, cols_for_Y,size=0.2,state=1):\n",
    "    \n",
    "    X =  mypandas.drop(cols_for_Y, axis=1)\n",
    "    Y = mypandas[cols_for_Y]\n",
    "    X.head()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size, random_state=state)\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy(), Y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 5)\n",
      "(320, 2)\n",
      "(80, 5)\n",
      "(80, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.53348059, 0.26190476, 0.0962963 , 1.        , 0.        ],\n",
       "       [0.13032545, 0.64285714, 0.05185185, 1.        , 0.        ],\n",
       "       [0.72563338, 0.66666667, 0.75555556, 0.        , 1.        ],\n",
       "       ...,\n",
       "       [0.5607873 , 0.78571429, 0.05925926, 0.        , 1.        ],\n",
       "       [0.32001191, 0.66666667, 0.47407407, 0.        , 1.        ],\n",
       "       [0.49381405, 0.28571429, 0.25185185, 0.        , 1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hacer aquí la normalizacion de los datos\n",
    "import csv\n",
    "data = pd.read_csv('datasets/Social_Network_Ads.csv', encoding='utf-8' )\n",
    "data\n",
    "data=one_hot_cols(data,['Purchased','Gender'])\n",
    "dataset= fit_cols(data,data.columns,Min_Max)\n",
    "Xtrain,X_test , Ytrain, Y_test = split_Dataset(data,['Purchased_0','Purchased_1'])\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "display(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE DE LA CAPA DE LA RED\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class neural_layer(object):\n",
    "    def __init__(self, n_conn, n_neur, activation=\"relu\"):\n",
    "        self.act = Funciones[activation]\n",
    "        self.activation=activation\n",
    "        self.random=self.act.Rand_init  \n",
    "        self.shape=(n_conn,n_neur)\n",
    "        self.Initialize()\n",
    "        \n",
    "    def show(self,Full=False):\n",
    "        print(f\"Pesos shape:{np.shape(self.W)} bias shape:{np.shape(self.b)} Activation:{self.activation}\")\n",
    "        print(f\"Activation:{self.activation}, Random:{self.random}\")\n",
    "        print(\"______________________\")\n",
    "        if Full:\n",
    "            print(f\"Pesos:\")\n",
    "            print(self.W)\n",
    "            print(\"#####\")\n",
    "            print(f\"Bias:\")\n",
    "            print(self.b)\n",
    "            \n",
    "    def Initialize(self):\n",
    "        #inicializa los pesos iniciales con aleatorios\n",
    "        self.b = np.random.uniform(*self.random,(1, self.shape[1]))      \n",
    "        self.W = np.random.uniform(*self.random,self.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "#CLASE RED NEURONAL MULTICAPA        \n",
    "class Neural_Net(object):\n",
    "    def __init__(self,Input,loss):\n",
    "        self.loss = Loss[loss]\n",
    "        self.Funcion_Loss=loss\n",
    "        self.Input=Input\n",
    "        self.NN=None;\n",
    "              \n",
    "    def Add_Layer(self,Num_neurons, function):\n",
    "        if self.NN is None:\n",
    "            self.NN=[]\n",
    "            self.NN.append(neural_layer(self.Input,Num_neurons,function))\n",
    "        else:\n",
    "            _,L_input=np.shape(self.NN[-1].W)\n",
    "            self.NN.append(neural_layer(L_input,Num_neurons,function))\n",
    "            \n",
    "    def Show_Model(self, Full=False):\n",
    "        print(f\"Input shape:{self.Input}, Loss: {self.Funcion_Loss}\")\n",
    "        for i,L in enumerate(self.NN):\n",
    "            print(F\"Layer_{i}:\")\n",
    "            L.show(Full)\n",
    "            \n",
    "            \n",
    "    # fucnción de predicción (fordware pass)    \n",
    "    def Predict(self,X):  \n",
    "      #sólo podemos pasar Numpy  \n",
    "      sx=np.shape(X)\n",
    "      X=X.reshape(1,sx[0])\n",
    "      if self.NN is None:\n",
    "          print(\"error in Predict Method ( not NEURAL network available)\")\n",
    "          return 0\n",
    "        \n",
    "      out = [(None, X)] #primer data necesario\n",
    "      # Forward pass\n",
    "      for l, layer in enumerate(self.NN):\n",
    "          z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "          a = self.NN[l].act.F(z)\n",
    "          out.append((z, a))\n",
    "      return out[-1][1]\n",
    "    \n",
    "    \n",
    "    # función retropropagación del error\n",
    "    def _backward_pass(self, X, Y,lr=0.01):\n",
    "      sx=np.shape(X)\n",
    "      sy=np.shape(Y)   \n",
    "      X=X.reshape(1,sx[0])\n",
    "      Y=Y.reshape(1,sy[0])\n",
    "\n",
    "      # Forward pass\n",
    "      out = [(None, X)] #primer data necesario\n",
    "      for l, layer in enumerate(self.NN):\n",
    "            z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "            a = self.NN[l].act.F(z)\n",
    "            out.append((z, a))\n",
    "\n",
    "      # Backward pass \n",
    "      deltas = []\n",
    "      for l in reversed(range(0, len(self.NN))):\n",
    "        z = out[l+1][0]\n",
    "        a = out[l+1][1]\n",
    "        if l == len(self.NN) - 1:\n",
    "            deltas.insert(0, self.loss.D(a, Y) * self.NN[l].act.D(a)) # La última capa\n",
    "        else:\n",
    "            deltas.insert(0, deltas[0] @ _W.T * self.NN[l-1].act.D(a))\n",
    "        _W = self.NN[l].W #los pesos en la capa superior\n",
    " \n",
    "        # Gradient descent. actualizamos pesos \n",
    "        self.NN[l].b = self.NN[l].b - (deltas[0]* lr)\n",
    "        self.NN[l].W = self.NN[l].W - (lr * (out[l][1].T @ deltas[0]))\n",
    "      return out[-1][1]\n",
    "\n",
    "    # función de entrenamiento de la red\n",
    "    def Train(self,X,Y,lr=0.01,epoch=10,batch_size=1):\n",
    "        H_loss = []\n",
    "        H_acc=[]\n",
    "        \n",
    "        # inicializamos las capas neuronales a valores ramdom del rango de la función\n",
    "        for Layer in self.NN:\n",
    "            Layer.Initialize()\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            account=0\n",
    "            epoch_Loss=0\n",
    "            epoch_Acc=0\n",
    "            # Entrenemos a la red! con el dataset de validación\n",
    "            for j in range(len(X)):\n",
    "                pY = self._backward_pass(X[j,:], Y[j,:],lr)#fila, fila, learning rate\n",
    "                epoch_Loss+=self.loss.F(pY[0],Y[j,:])\n",
    "                if (Y[j,:]==np.round(pY)).all():#condicion de acertar\n",
    "                    epoch_Acc+=1\n",
    "            H_acc.append(epoch_Acc/len(Y)*100)    \n",
    "            H_loss.append(epoch_Loss/len(Y))#media del error\n",
    "            \n",
    "            #imprimimos por pantalla resultados\n",
    "            print(\"Epoch={}, Accurary={} Loss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7)))\n",
    "            clear_output(wait=True)\n",
    "        print(\"Epoch={}, Accuracy={} Loss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7)))\n",
    "        return H_loss,H_acc\n",
    "\n",
    "    \n",
    "# VISUALIZACIÓN Y TEST\n",
    "def Show_Loss_Acc(H_loss,H_acc):\n",
    "    plt.plot(range(len(H_loss)), H_loss,\"tab:blue\")\n",
    "    plt.ylabel(\"loss function \")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "    plt.plot(range(len(H_acc)), H_acc, \"tab:red\")\n",
    "    plt.ylabel(\"ACCURACY\")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "       \n",
    "def print_predict(neural_net,X,Y):\n",
    "    for i in range(len(X)):\n",
    "        sal_float=neural_net.Predict(X[i])\n",
    "        sal=np.round(sal_float)\n",
    "        \n",
    "        if (Y[i]==np.round(sal)).all():\n",
    "            print(\"Input:{}-- Real:{} predict: {} predict_float:{}\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "        else:\n",
    "            print(\"\\x1b[31m Input:{}-- Real:{} predict: {} predict_float:{}\\x1b[0m\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINIMOS LOS MODELOs\n",
    "def Model1():\n",
    "    red=Neural_Net(Input=5,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(10,\"relu\")\n",
    "    red.Add_Layer(2,\"softmax\")\n",
    "    return red\n",
    "\n",
    "def Model2():\n",
    "    red=Neural_Net(Input=5,loss=\"mse\")\n",
    "    red.Add_Layer(200,\"tanh\")\n",
    "    red.Add_Layer(200,\"tanh1\")\n",
    "    red.Add_Layer(2,\"sigm\")\n",
    "    return red\n",
    "\n",
    "def Model3(): \n",
    "    red=Neural_Net(Input=5,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(10,\"sigm\")\n",
    "    red.Add_Layer(20,\"tanh\")\n",
    "    red.Add_Layer(8,\"relu\")\n",
    "    red.Add_Layer(2,\"softmax\")\n",
    "    return red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:5, Loss: mse\n",
      "Layer_0:\n",
      "Pesos shape:(5, 200) bias shape:(1, 200) Activation:tanh\n",
      "Activation:tanh, Random:(-1, 1)\n",
      "______________________\n",
      "Layer_1:\n",
      "Pesos shape:(200, 200) bias shape:(1, 200) Activation:tanh1\n",
      "Activation:tanh1, Random:(-1, 1)\n",
      "______________________\n",
      "Layer_2:\n",
      "Pesos shape:(200, 2) bias shape:(1, 2) Activation:sigm\n",
      "Activation:sigm, Random:(0, 1)\n",
      "______________________\n",
      "Input shape:5, Loss: mse\n",
      "Layer_0:\n",
      "Pesos shape:(5, 200) bias shape:(1, 200) Activation:tanh\n",
      "Activation:tanh, Random:(-1, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[ 1.99225280e-01 -3.66435725e-01 -8.27044764e-01  7.11816303e-01\n",
      "   4.74015055e-01  9.99145654e-01 -4.94116725e-01 -7.45140794e-01\n",
      "   8.42820899e-02 -4.06400636e-01 -5.25582975e-01 -6.98398231e-01\n",
      "   3.44995501e-01  9.54662367e-01  9.37396232e-01 -9.26153751e-01\n",
      "  -9.60788333e-01  7.66790792e-01  1.14108883e-01 -7.73272986e-01\n",
      "  -1.22505495e-01 -1.81730778e-01 -6.97792704e-01  7.21902544e-01\n",
      "   5.75976095e-01 -6.79936378e-01  9.69005239e-01  5.82932411e-01\n",
      "  -4.25183458e-01 -4.26511213e-01 -1.64550419e-01  3.54843453e-02\n",
      "  -6.11023191e-01 -6.61789462e-01  4.15550727e-01  2.81210697e-01\n",
      "  -6.61107116e-01 -8.74748350e-01 -9.08924224e-01 -3.69390216e-01\n",
      "  -7.92420136e-01 -8.47555068e-01 -4.92660098e-01 -5.54351994e-01\n",
      "  -4.16132400e-01  2.42791037e-01 -9.67716059e-01 -2.82212919e-01\n",
      "  -9.96220645e-01  5.26142680e-01 -8.51131179e-01  8.87198133e-01\n",
      "  -4.20109256e-01 -5.83820833e-01 -3.79110610e-01 -2.10310472e-01\n",
      "  -4.38455815e-01  9.70572310e-01 -8.32858922e-01 -6.41628361e-01\n",
      "   3.92629458e-01 -7.85874350e-01 -8.38801981e-01 -8.61931718e-01\n",
      "  -1.24521274e-01 -9.03920839e-01 -7.32562676e-01 -5.47089061e-01\n",
      "  -4.95277151e-01 -2.36436498e-01  5.79847120e-01  6.93330470e-01\n",
      "   3.88283105e-01 -2.39110892e-01 -9.63845445e-01  2.51016566e-01\n",
      "   4.96177967e-01 -1.53741165e-01 -9.42381594e-01  1.47491431e-01\n",
      "   7.24521060e-02 -6.24902177e-01  7.67287134e-01  9.33361489e-01\n",
      "   9.14058273e-02  4.27929872e-01 -2.31433539e-03 -3.61387905e-02\n",
      "   9.08565589e-01  2.55989988e-01  1.36491348e-01  5.26009124e-01\n",
      "   8.89698256e-01 -5.83005943e-01  1.08172832e-01  4.82096981e-01\n",
      "  -8.46893306e-01  7.23362333e-02  7.22661352e-01 -8.27222650e-02\n",
      "   6.75240779e-01  5.90724106e-01 -1.79380615e-01  9.43282246e-01\n",
      "  -5.04406471e-01  4.10276113e-01  4.83152143e-01  5.92050350e-02\n",
      "   7.37731494e-01 -4.56588901e-01  7.69519267e-01 -5.68556496e-01\n",
      "   2.33072048e-02 -8.29750436e-01 -4.26949274e-01 -4.82207827e-01\n",
      "  -7.70486237e-01 -6.28917274e-02 -9.09274209e-01 -5.54754735e-01\n",
      "   6.76077048e-01  5.08094538e-01  6.29910303e-01  7.00419886e-02\n",
      "  -3.16587427e-01  9.12185302e-01  4.74107867e-01 -5.37629407e-01\n",
      "  -1.96996278e-01  8.67569765e-01  9.77112146e-01 -5.37234574e-01\n",
      "   3.75472466e-02  5.35904950e-02 -1.93493482e-01  3.72226063e-01\n",
      "   2.72857977e-01 -3.62272074e-01 -4.11021344e-01  6.88640814e-01\n",
      "  -6.38667104e-01 -6.67996666e-01 -5.97664956e-01 -1.92211944e-01\n",
      "  -6.22811141e-01  9.13679505e-01 -8.06062938e-01 -3.35010539e-01\n",
      "   5.27088822e-01  6.95157172e-01 -3.14504013e-01  7.52451672e-01\n",
      "   2.37363675e-01 -4.76736108e-01 -1.89723417e-01  8.85218086e-02\n",
      "  -5.91571323e-01 -8.60385293e-01 -5.92386978e-02 -8.48858161e-01\n",
      "   6.85576315e-01  8.36758821e-01 -7.21576533e-02 -7.43166267e-01\n",
      "  -3.29921683e-02  2.16281615e-01  4.42723855e-01 -1.85177127e-01\n",
      "  -7.09150532e-01  9.95625551e-01 -4.36174931e-01  6.32044765e-01\n",
      "  -9.13962706e-01 -3.30809427e-01 -1.54597398e-01 -6.24573064e-01\n",
      "  -4.06684503e-01  3.00969193e-01 -3.86095780e-02 -3.12696938e-01\n",
      "  -1.41142488e-01  4.70261570e-01 -8.56264352e-01  8.94820992e-01\n",
      "   9.69272836e-01 -5.90254146e-01 -4.94940614e-01  8.28751984e-01\n",
      "  -4.82485074e-01  5.75110562e-01 -3.96241873e-01  3.65897312e-01\n",
      "   8.70696011e-01  3.74043007e-01 -4.26785134e-01 -8.21280505e-01\n",
      "   8.39726237e-01  4.26297063e-01 -1.52144994e-01 -6.50007909e-01]\n",
      " [-7.55499140e-01  9.30278674e-01  1.74727260e-02  3.99370174e-01\n",
      "  -5.45515360e-01  2.23734667e-01  8.08696108e-01 -3.81878663e-01\n",
      "  -9.45582680e-01  2.11982616e-01  1.44582964e-01 -5.69585166e-01\n",
      "  -2.48306582e-01 -1.72139912e-02 -2.90144834e-01  3.53977860e-01\n",
      "   4.39850259e-01 -8.66732406e-01 -5.46565746e-01 -9.60091270e-01\n",
      "  -3.99053465e-01 -5.72723925e-01  4.94549007e-01 -8.24380317e-01\n",
      "   7.09417062e-01  2.22200441e-01  9.83320377e-01 -5.55536149e-01\n",
      "   5.31674941e-01  7.62675513e-01  2.51082865e-01  5.48390156e-01\n",
      "   9.26165949e-01  1.13074039e-02 -1.17266128e-01  3.91075457e-01\n",
      "   3.60851172e-01  8.53089549e-01  1.96648963e-02  1.95863387e-02\n",
      "  -5.93093577e-01 -3.10566334e-01 -2.31956091e-01  4.62414414e-01\n",
      "   8.49292221e-01  3.86460185e-01 -7.96122467e-01 -4.92864142e-01\n",
      "  -8.98483340e-01  4.95717933e-01 -4.54909044e-01  1.45535854e-01\n",
      "   4.02772151e-01  6.64156417e-01 -9.31541660e-01 -6.62991294e-01\n",
      "  -5.40867490e-01  1.18231133e-01 -8.74836197e-01  2.98637848e-01\n",
      "   5.71975001e-01 -7.54257501e-01 -3.39628055e-01 -3.34837266e-01\n",
      "  -9.63498817e-01  8.88465937e-01  2.32526895e-01  9.79772299e-01\n",
      "  -4.89663857e-02  8.82126916e-01  5.38505301e-01  4.88210017e-01\n",
      "   6.37125434e-01 -1.88592298e-01 -7.50175896e-01 -1.15140140e-01\n",
      "  -6.54886440e-01 -4.39201512e-01 -2.88529977e-01 -6.73014537e-01\n",
      "   5.46222956e-01  4.12349630e-01  7.02586241e-01  1.14849510e-01\n",
      "  -4.94811562e-01  6.80191157e-01 -9.89339846e-02 -9.83289725e-01\n",
      "   8.52509812e-01  3.87910151e-02  3.72394853e-01 -6.77996894e-01\n",
      "   6.19991682e-01 -1.92758848e-02 -3.35414532e-01 -4.44311675e-01\n",
      "  -3.33035308e-01  4.02090579e-01 -2.38630098e-01  8.87235044e-01\n",
      "  -3.23524879e-01 -6.50860943e-01 -8.59321477e-01  7.82833041e-01\n",
      "   7.46012452e-01 -1.80640606e-01 -1.31400693e-01 -6.49232592e-01\n",
      "  -3.86205846e-02 -5.08194823e-01 -5.92264556e-01  4.53144218e-01\n",
      "   7.03387781e-01  4.81801733e-01  7.62269004e-01 -3.05095460e-01\n",
      "  -3.20701587e-01  2.23469383e-01  6.92368820e-01 -4.92660759e-01\n",
      "  -5.24988967e-01  6.57074295e-01 -2.54440661e-01  9.21713758e-01\n",
      "   4.36659481e-01  5.38534921e-01 -7.04362037e-01 -6.24115763e-02\n",
      "   1.69528717e-01  5.69257687e-01  9.04156183e-01 -2.91954626e-01\n",
      "  -7.20416617e-01 -9.28009526e-01 -3.33021985e-01 -2.70304641e-01\n",
      "  -8.52982209e-01 -7.70787936e-02  6.80919245e-02 -7.06027448e-01\n",
      "  -6.93111988e-01  2.79839861e-01  9.21534916e-01  2.01983772e-01\n",
      "  -1.56866685e-01  6.15223842e-01  3.98252015e-01  6.10016965e-01\n",
      "   5.70542758e-01  8.56977578e-01 -5.44166580e-02 -3.06802508e-01\n",
      "   1.71134373e-01 -5.13388719e-01 -1.21415870e-01  7.76926426e-01\n",
      "   7.74937860e-01 -1.30103235e-01  9.33467776e-01  4.56179886e-01\n",
      "  -1.43682588e-01 -1.25372703e-01 -3.28980626e-01 -2.83045450e-01\n",
      "   1.79465207e-01  8.07078184e-01 -5.20557940e-02  9.94821441e-01\n",
      "   1.92033050e-01  1.15995441e-01 -6.29588340e-01 -6.81221382e-01\n",
      "   4.36512854e-01 -1.53890359e-02  5.81335571e-02  3.33792797e-01\n",
      "  -7.69852277e-01  9.41075697e-01  1.61686023e-01 -4.45799737e-01\n",
      "  -3.48156467e-01  1.04085872e-01  3.97754641e-01  1.61267898e-01\n",
      "   5.79498463e-01 -3.51209235e-01 -6.69590572e-01 -1.25290497e-01\n",
      "  -1.67602248e-01  5.20812832e-01  9.70611701e-01  8.29446901e-01\n",
      "  -8.09081985e-01 -6.31330586e-01  1.88885035e-02 -5.83839621e-01\n",
      "   1.51577206e-01  9.41789429e-01  8.01613818e-01  1.12636588e-01]\n",
      " [-2.85417265e-01  7.00999733e-01  2.87252698e-02  2.62038636e-01\n",
      "  -7.45420836e-01 -2.59504082e-02  2.94883437e-01  2.19442908e-01\n",
      "  -8.68079558e-01 -9.24502701e-01  7.37322413e-01  9.49476187e-01\n",
      "  -4.61782534e-01 -3.30757780e-01 -8.09170155e-01  4.07746433e-01\n",
      "   8.33274065e-01 -3.43582249e-01 -2.93060841e-01 -1.97863165e-01\n",
      "  -6.58129194e-01  3.92412764e-01 -4.20380396e-01 -4.22012465e-01\n",
      "  -6.99041115e-01 -4.19786816e-01  2.17308906e-01 -3.14248983e-01\n",
      "   8.31381347e-01  5.58958948e-01  9.81616728e-01 -8.32756664e-01\n",
      "  -9.23812968e-01  4.37707121e-01  4.60181719e-01  9.94138827e-01\n",
      "  -1.03913911e-01  6.12359479e-02 -8.46551760e-01 -7.14982801e-01\n",
      "  -4.21919840e-01 -1.80135215e-01  3.14011003e-01  3.32432615e-02\n",
      "  -5.16159564e-01  7.45086598e-01  7.78867777e-01 -2.28135513e-01\n",
      "  -2.02304712e-02  8.91664884e-01 -6.59555078e-01 -2.99218762e-01\n",
      "  -8.86273614e-01 -2.62667617e-01  9.99242323e-03 -9.48223295e-01\n",
      "   9.11663184e-01 -8.95044069e-01 -2.61128044e-01 -5.21790874e-01\n",
      "  -5.75015177e-02 -3.32326853e-01  3.78276205e-01  2.68483439e-01\n",
      "   2.32870584e-01 -9.42585893e-01  7.48931984e-01 -6.55414678e-01\n",
      "  -9.77465487e-01 -6.17790187e-01  8.60375794e-01 -7.02511970e-01\n",
      "   1.52073641e-01 -5.48566893e-01  1.65255659e-01 -2.73082103e-01\n",
      "   7.02093232e-01 -1.02833786e-01  4.34882127e-02  2.61298217e-01\n",
      "   4.46502061e-02 -2.07180914e-01 -2.29770212e-01 -7.88488701e-01\n",
      "   5.38087640e-01  9.28987586e-01 -2.88791167e-01  7.76182459e-01\n",
      "  -3.80408457e-01  5.58763307e-01  7.78026350e-01  6.98929131e-01\n",
      "  -6.42645928e-01 -1.96625475e-01 -4.42809960e-01 -5.67688239e-01\n",
      "  -3.30763408e-01 -9.51775784e-01 -1.36540905e-01 -8.15354523e-01\n",
      "  -4.73088010e-02 -4.51499991e-02  3.53457706e-01 -4.61098909e-01\n",
      "   2.41537913e-01  8.57521150e-01  6.29803355e-01  9.96940362e-02\n",
      "   8.51307874e-01 -9.40734779e-01  4.23872291e-01 -6.73016620e-01\n",
      "  -3.33503216e-01 -5.66360377e-01  1.65879087e-01 -7.74879252e-02\n",
      "   5.91726532e-01 -1.18742606e-01 -9.12564377e-01 -6.52801041e-01\n",
      "  -3.56934764e-01  6.42908127e-01 -3.63831730e-01  7.33809238e-01\n",
      "  -1.90057886e-01  2.06767466e-01 -7.49392447e-01 -9.78692632e-01\n",
      "  -1.14002617e-01  7.70103182e-01 -1.81669867e-01 -8.13280566e-02\n",
      "   3.70439812e-01 -1.97494875e-01  4.84849665e-01  4.90754378e-01\n",
      "  -9.59550728e-01 -7.37001309e-01 -5.47760701e-01 -5.59795311e-01\n",
      "   1.84501816e-01 -1.10973256e-01 -4.12279873e-01 -3.51222622e-01\n",
      "   6.13020835e-02  9.49497088e-01 -3.45510355e-02  5.77070130e-01\n",
      "  -1.47531667e-01  7.53625935e-01 -9.19869389e-01  2.21619906e-01\n",
      "  -6.59779799e-01 -1.09609962e-01 -1.70911870e-01  6.55638925e-01\n",
      "   1.80694024e-01  5.77974710e-01 -8.00643899e-01  2.38134651e-01\n",
      "  -8.19308382e-01  9.47781575e-01  2.56092562e-01  7.88599590e-01\n",
      "   2.47226461e-01  4.89603137e-01  7.20628929e-01  2.93899553e-01\n",
      "  -5.19310220e-01 -1.53215478e-01 -6.51337997e-01 -7.96894425e-01\n",
      "  -3.85050579e-01  6.38330286e-01  9.38312442e-01  7.57841893e-01\n",
      "  -5.36505248e-01  3.18294061e-01 -3.83664363e-01 -8.30893163e-01\n",
      "   1.79870642e-01  3.26534225e-01 -5.39427810e-01 -5.38537295e-02\n",
      "  -1.43214217e-01 -5.38945056e-01 -4.82336866e-01 -3.99847666e-01\n",
      "  -6.00652476e-01  2.71128601e-01  6.71856084e-01  2.06094702e-01\n",
      "   6.69717714e-01 -9.63717398e-02 -9.80725345e-01 -1.29800510e-01\n",
      "   4.36189620e-01  4.82150798e-01 -2.01972956e-01 -1.97648177e-01]\n",
      " [-5.68571716e-01 -5.68162041e-01 -9.68153760e-01  9.34907237e-01\n",
      "   9.75930626e-01  2.22163301e-01  9.95797707e-01  3.08095817e-01\n",
      "  -5.76114639e-02 -9.62483517e-01 -3.72659411e-01  3.62876189e-01\n",
      "  -5.67835339e-01  5.43251721e-01  3.14227607e-01  2.61883358e-02\n",
      "  -5.81031339e-01  2.36440899e-01  3.66064279e-01 -4.87933378e-01\n",
      "  -6.71615102e-01 -5.62112630e-01 -1.80905233e-01 -7.17835494e-02\n",
      "   9.60227312e-01 -2.77070994e-01 -7.57325172e-01  5.39791618e-01\n",
      "   6.55001401e-01 -2.37776373e-03  7.75097677e-01  9.40726439e-02\n",
      "   4.43741009e-01  4.78166343e-01  9.93487343e-01 -5.45771311e-01\n",
      "  -9.78279251e-01 -6.53495547e-01  7.85625543e-02 -6.27320129e-01\n",
      "   4.09884921e-01 -9.41662684e-01 -2.98594497e-01  7.22263060e-01\n",
      "  -9.95560819e-01 -5.51632614e-01 -4.78510536e-01 -8.02275911e-01\n",
      "   4.35927844e-01 -6.39452170e-01 -5.39199612e-01 -1.02085155e-02\n",
      "   3.09379755e-01  1.90469514e-01 -5.61713386e-01  6.05424654e-01\n",
      "   6.65754233e-01 -9.04606880e-01 -7.43590538e-01 -1.10343548e-01\n",
      "   8.35414881e-01 -3.15436463e-01 -3.91686793e-01  7.48298203e-01\n",
      "  -4.31285167e-01 -6.59581388e-01  5.41182793e-01  9.53703261e-01\n",
      "  -3.55196464e-01 -2.33974465e-01 -2.77758258e-01 -2.81918544e-01\n",
      "   1.12960888e-01 -1.21336543e-01 -8.73180501e-01 -6.65016262e-01\n",
      "  -7.89660647e-01  1.07226815e-01 -4.85386128e-01 -8.31515031e-01\n",
      "   1.25052234e-01  5.14999956e-01 -8.54522042e-01  7.12700579e-01\n",
      "   6.30387844e-01  6.77037612e-01  5.10791424e-01 -3.51100571e-01\n",
      "  -1.26761248e-01 -2.85724588e-01  6.94107627e-01  8.71399616e-01\n",
      "   2.15718439e-01  5.27704470e-01  8.55156452e-02  8.63754754e-01\n",
      "  -6.19432172e-01  4.07838850e-02 -1.06063438e-01 -7.38276216e-01\n",
      "  -4.86157325e-01 -7.41091017e-01 -4.25400545e-02  3.51087294e-01\n",
      "  -6.53602275e-01  4.94117894e-01  7.80472393e-02  4.62168730e-01\n",
      "   8.85015012e-01 -9.08311243e-01  9.19733007e-01 -1.97600381e-01\n",
      "  -7.12037747e-01  4.67721721e-02 -3.72405937e-01  9.47123532e-01\n",
      "   4.20746346e-01 -8.60136069e-02 -4.45535148e-01  6.09326777e-01\n",
      "   8.95388131e-01 -2.18795764e-01 -3.01029024e-01  6.65098112e-01\n",
      "   4.22972728e-01 -3.69842469e-01 -3.44911600e-01  4.13683409e-01\n",
      "   3.13730779e-01 -5.32025284e-01 -1.50119452e-01 -8.56562531e-01\n",
      "  -8.69830302e-02  3.80114689e-01  1.30109605e-01 -7.85563356e-01\n",
      "   8.23268386e-01 -2.67885592e-01 -7.09799601e-01 -6.82976327e-01\n",
      "  -1.40577331e-01 -6.81250177e-01 -8.81178258e-01  4.35862210e-01\n",
      "  -1.60273111e-02  8.51198764e-01 -3.04396010e-02 -8.48770687e-02\n",
      "   1.13565932e-01 -2.13363446e-01 -6.04243042e-01  2.96573929e-01\n",
      "  -2.50517760e-01  7.49612494e-01  5.79642766e-01 -6.83950757e-01\n",
      "   6.93699231e-01 -6.15381018e-01  9.60918933e-01 -2.59002748e-01\n",
      "   4.76269611e-03  8.54825822e-01  1.31166978e-01 -9.57261155e-01\n",
      "   3.62350775e-01 -3.43495176e-01 -9.17686032e-01  1.50426415e-01\n",
      "   3.56889229e-01  2.94212095e-01 -1.52361900e-01  7.99791869e-01\n",
      "   8.70887268e-02  3.35736035e-01 -4.65283229e-01  9.42205176e-01\n",
      "   3.78823196e-02 -5.80757996e-01  7.83690286e-01  2.58968778e-01\n",
      "  -1.70465297e-01  3.69438345e-01  3.10664163e-01 -7.53106793e-01\n",
      "   5.83150133e-01 -7.48000887e-02  5.37692656e-01  8.38872973e-01\n",
      "  -4.53655538e-01  7.56723129e-02  8.83937687e-01  3.75997443e-01\n",
      "   1.95272713e-01 -2.08772874e-01  6.69712698e-03  2.21117235e-01\n",
      "   8.45841686e-01 -1.13786992e-01 -5.88290212e-01 -9.11596452e-01]\n",
      " [-7.24121335e-01 -8.59557977e-01 -4.54919996e-01 -7.96744050e-01\n",
      "  -7.00454375e-01 -2.50627271e-01 -7.78815770e-01  7.97645865e-01\n",
      "   2.58879079e-01  7.51130556e-01  7.42305208e-01  4.07019874e-02\n",
      "   1.35665506e-01 -7.77664519e-01  4.59015466e-01 -6.55527077e-01\n",
      "  -7.32250704e-01  2.62374677e-01  7.72003551e-01  6.04098283e-01\n",
      "  -8.91213280e-01 -2.85057787e-02 -1.70044424e-02  6.02794841e-01\n",
      "   8.39326015e-01  7.51306972e-01  3.97702183e-01  9.19311934e-01\n",
      "  -3.42385792e-01  8.02960190e-01 -1.77822934e-01  5.30923438e-01\n",
      "   1.61619455e-02 -3.71717864e-01 -8.83192888e-02  6.81030482e-01\n",
      "   6.67611396e-01 -1.08899848e-01  2.17886982e-01  7.90216201e-01\n",
      "  -8.19210155e-01 -1.31625435e-01 -4.00001568e-01 -3.92006469e-01\n",
      "  -8.10968748e-01  6.84530365e-01  1.67989946e-01  1.13479555e-01\n",
      "   2.11250317e-01  4.29396081e-01 -3.87450383e-01 -1.82809289e-02\n",
      "  -1.56436024e-01  5.90655439e-01 -5.13420307e-01  7.76502881e-01\n",
      "  -3.81854319e-01 -7.93363257e-01  6.59520147e-01 -2.97862407e-01\n",
      "   5.62927967e-01 -1.77400752e-01 -1.45200330e-01  3.96056966e-01\n",
      "  -3.80232823e-01  5.47952111e-01  9.98279410e-01 -7.57016581e-01\n",
      "  -4.10697320e-02 -8.17869710e-01 -7.79494701e-01  1.90637198e-01\n",
      "  -5.32404305e-01  9.16242246e-01  2.45417976e-01  3.76711920e-03\n",
      "  -5.63747338e-01 -5.37304653e-01  7.48065805e-01 -5.07941571e-01\n",
      "   1.16381192e-01  4.43921191e-01 -3.55932750e-01  7.88824331e-01\n",
      "  -4.01397459e-01 -7.45185199e-01  1.56601089e-04 -5.78507337e-01\n",
      "  -2.48850851e-02 -6.18982051e-01  5.12824709e-01 -5.36225329e-01\n",
      "   9.71636774e-02  3.81043095e-01  8.48849476e-01  6.39679353e-01\n",
      "   8.96336387e-01  9.51899139e-01 -4.11481803e-01  2.65624395e-01\n",
      "   8.93377129e-01  5.20133904e-01 -1.96163510e-01 -6.49342380e-01\n",
      "  -6.43286094e-01 -5.23274635e-01 -7.39617653e-01 -3.19966625e-01\n",
      "   8.59220041e-01 -6.38814721e-01  1.00703417e-02 -3.75199877e-01\n",
      "   8.73641967e-01  4.34104097e-01  8.08198670e-01  8.90176475e-01\n",
      "   7.46669711e-01  5.70902850e-01  3.77676111e-01  9.92919660e-01\n",
      "   2.30666237e-01 -3.72132206e-01  1.68151807e-02  3.12887380e-01\n",
      "  -3.28957526e-01 -5.28642865e-02 -2.24998472e-01  7.11614644e-01\n",
      "   7.93868784e-01 -1.05992353e-01 -4.57042097e-01 -6.56880373e-01\n",
      "   5.41811633e-01  9.17504583e-01 -9.40258849e-01 -3.30674521e-03\n",
      "  -5.67578772e-01 -4.51929137e-01  4.70895768e-01 -8.14631129e-01\n",
      "   7.03354276e-01  6.05737194e-01  9.33746474e-01  5.51233451e-01\n",
      "  -7.34587043e-01 -1.58144827e-01 -7.85922220e-01  5.83795434e-01\n",
      "  -8.83273501e-02  8.15547885e-01  2.78314239e-01  5.94557178e-01\n",
      "   4.56547017e-01  8.52136408e-01 -5.90257019e-01  7.90307030e-01\n",
      "  -8.22426968e-01  4.39628409e-01 -1.44065824e-01 -8.64366799e-01\n",
      "   1.32335484e-01  3.81446894e-02 -4.78927020e-01  7.95166918e-01\n",
      "  -7.07424391e-01 -3.98430285e-01 -3.91629362e-01  8.11739694e-01\n",
      "  -1.91789608e-01  9.63090498e-01 -1.13185345e-01  1.90354436e-01\n",
      "   1.74735824e-02 -6.38581872e-01 -9.91980130e-01  9.37812018e-01\n",
      "  -8.81842458e-01 -9.42712632e-01  5.92556262e-01  4.13107531e-01\n",
      "   9.07063096e-01  2.40692159e-01  2.02323227e-01  4.98032054e-01\n",
      "  -7.57141063e-01  8.14287967e-01 -8.87179163e-01  3.06350373e-01\n",
      "   8.47211597e-01 -9.87511320e-01 -6.94245751e-01 -5.99638446e-01\n",
      "   5.39521245e-01 -4.04696972e-02 -6.87572463e-01  4.74608457e-01\n",
      "   9.50624155e-01  1.78860856e-01  3.43688044e-01  6.00738343e-02]]\n",
      "#####\n",
      "Bias:\n",
      "[[ 0.19612724 -0.50762726 -0.97657489  0.92772615  0.91466889 -0.48962489\n",
      "   0.49532095  0.12356891 -0.01993272 -0.37229952  0.81119709 -0.5821767\n",
      "  -0.64538741  0.21150485 -0.69842911  0.44704056  0.6765585  -0.67532987\n",
      "  -0.44469503 -0.56970887  0.93906143 -0.72942976  0.83825579  0.07503866\n",
      "  -0.62010122  0.55797149  0.44591607  0.81760082 -0.506685    0.80492924\n",
      "   0.98900794 -0.61240878 -0.75961229 -0.3613973  -0.65317333 -0.64268744\n",
      "  -0.35665707 -0.98908446 -0.85608371  0.05246909 -0.20996109  0.35289265\n",
      "   0.42380106  0.6477425   0.84229506  0.54726857 -0.36213771 -0.00114228\n",
      "  -0.18094799  0.17678431  0.25429647 -0.4674096  -0.0580087  -0.4750538\n",
      "  -0.5712862   0.3019049  -0.65271323  0.49198101  0.02165898 -0.33191863\n",
      "   0.4109613  -0.09412251  0.71968731 -0.77202375 -0.65391732 -0.7253165\n",
      "   0.13218907  0.40443494  0.17500994 -0.54641754 -0.82179077 -0.85383888\n",
      "  -0.68841438  0.07084673  0.09798114 -0.062363    0.61543048  0.88729312\n",
      "   0.17045904 -0.96055558 -0.25869367  0.45916516 -0.12889357 -0.2478676\n",
      "   0.26445501  0.00325804  0.09011832  0.01395369  0.13559362 -0.5686352\n",
      "   0.33467251  0.28059772  0.41883791 -0.00855657 -0.61825196 -0.30813609\n",
      "  -0.42629651  0.96850308  0.43381379  0.20689626 -0.77340598 -0.00448691\n",
      "   0.45994982 -0.39121652  0.20810239  0.56812424 -0.63518126 -0.7362112\n",
      "   0.95870092  0.19820858 -0.86469162  0.18732468 -0.14596195 -0.90688813\n",
      "  -0.52581671 -0.77316522 -0.05678639  0.56071703 -0.03033115 -0.85478679\n",
      "  -0.22672175  0.80753057  0.30585826  0.9329414  -0.24869972 -0.10747725\n",
      "   0.52802146 -0.28632969  0.59881245 -0.20482463 -0.90146442  0.83126469\n",
      "  -0.18256619  0.06546845  0.81904832 -0.28040901 -0.28071442  0.20728021\n",
      "   0.4934495   0.95250502 -0.99795175 -0.72901169  0.91520776 -0.47180164\n",
      "  -0.51243416  0.53687917 -0.45138444 -0.66463145 -0.70647289  0.51134791\n",
      "   0.53347569  0.55722679  0.91056993  0.59250628 -0.03787245 -0.09726529\n",
      "   0.12406083  0.43095743  0.13800916 -0.7365762  -0.62321553 -0.15101064\n",
      "  -0.39356059  0.95167672 -0.857848    0.82081274 -0.11690648  0.64425552\n",
      "  -0.90592369 -0.20277562 -0.32307269  0.41997371  0.89747575  0.85444962\n",
      "   0.45313205  0.83297231 -0.78580311 -0.97349502 -0.07077358 -0.11617626\n",
      "  -0.25169855  0.8847212  -0.98205833  0.84432798 -0.961478   -0.09519467\n",
      "   0.06431601 -0.41431677  0.40959698 -0.11155171  0.28792951  0.65800219\n",
      "  -0.89665489 -0.87419398 -0.20993252 -0.51423362  0.55363848 -0.17480313\n",
      "  -0.24585056  0.15786175]]\n",
      "Layer_1:\n",
      "Pesos shape:(200, 200) bias shape:(1, 200) Activation:tanh1\n",
      "Activation:tanh1, Random:(-1, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[-0.7242489   0.67680822  0.65870599 ... -0.58587337  0.45324119\n",
      "   0.14088258]\n",
      " [ 0.72555703 -0.97282801 -0.2461009  ... -0.60766984  0.42726689\n",
      "   0.39659186]\n",
      " [ 0.11148722  0.60747076  0.24404697 ...  0.26727208 -0.34400143\n",
      "   0.15525186]\n",
      " ...\n",
      " [ 0.76479766 -0.47766441  0.59851598 ...  0.55633316 -0.68575967\n",
      "   0.34644682]\n",
      " [-0.23702754  0.14888175  0.94198726 ...  0.40385379 -0.03336389\n",
      "   0.10862889]\n",
      " [ 0.587935    0.22456965  0.97237705 ... -0.64044273  0.92838213\n",
      "  -0.00344352]]\n",
      "#####\n",
      "Bias:\n",
      "[[ 3.33339758e-01  3.28475049e-01 -9.43868913e-01 -7.21716087e-01\n",
      "  -1.40423235e-02  4.14621946e-01  1.57642990e-01 -4.85361503e-01\n",
      "   5.69395875e-01  6.68395180e-02 -3.51016688e-01  1.07418654e-01\n",
      "  -1.92359025e-01 -2.00106060e-01 -6.69172622e-01  1.77868776e-01\n",
      "   4.41087264e-01 -9.96724409e-01 -1.71512237e-01 -1.71228916e-01\n",
      "  -7.38677091e-01 -8.14084674e-01 -5.08558807e-01 -5.27290211e-02\n",
      "  -1.82031202e-01 -8.06191049e-01  2.32663458e-01 -6.39384036e-01\n",
      "  -4.61740005e-01 -2.31693947e-01 -3.12904733e-01  5.53423172e-01\n",
      "   9.03311312e-01  2.99526148e-01 -8.86905470e-01 -6.74880310e-01\n",
      "  -8.13184587e-01  4.72797369e-01 -1.82852777e-01 -3.47536479e-01\n",
      "  -9.24846173e-01 -8.82704595e-01  5.93919529e-01  4.76653965e-01\n",
      "   9.94632463e-04 -4.03304322e-01 -9.75963645e-02  7.01773860e-01\n",
      "   7.72760127e-01  9.27104034e-01 -7.56022141e-02  4.57838126e-01\n",
      "   8.78224040e-01  7.38719422e-01 -4.22521534e-02  5.40515724e-01\n",
      "   6.07484647e-01 -3.97953969e-01  9.94116101e-01 -5.27792324e-01\n",
      "  -5.16945957e-01 -7.02617592e-01  4.51378711e-01 -6.67644706e-01\n",
      "   8.21660406e-01 -4.30895645e-01  6.62476199e-01  5.77215134e-01\n",
      "   8.85597095e-01  3.97250339e-03 -9.38269308e-02 -6.00047400e-01\n",
      "  -3.27888401e-01  6.00843324e-02 -2.82157718e-01  4.53337700e-01\n",
      "   2.59325101e-01 -8.76479334e-01  7.47509033e-01 -4.65279304e-01\n",
      "   3.30760209e-01  8.95381489e-01 -4.37876875e-01  6.29530452e-01\n",
      "   8.33860563e-01 -5.19515611e-01 -2.79889749e-01  5.74652652e-01\n",
      "  -4.95501654e-01 -8.32195949e-01  4.66543224e-01 -8.29616636e-01\n",
      "   8.58706959e-01  2.21244131e-01  4.23512978e-01  7.34070978e-01\n",
      "  -2.94183851e-01  2.94952078e-01  7.38577191e-01 -2.57813683e-01\n",
      "   1.61324777e-02  2.11154541e-01  5.22488288e-01 -2.60445419e-01\n",
      "   1.60954801e-01  9.10626647e-01  1.05364422e-01 -4.25700230e-01\n",
      "   4.00991082e-01 -9.48663758e-01  3.28646081e-01  6.86005763e-01\n",
      "  -1.47057305e-01 -5.54585348e-01  6.99416080e-01  8.35282563e-01\n",
      "  -1.93698883e-01 -9.17372367e-01  3.20769014e-01 -5.20077712e-01\n",
      "   1.04465284e-01 -5.67333545e-01  3.72950468e-01 -8.28035917e-01\n",
      "  -7.69072234e-01  9.00922059e-01 -3.30201105e-01  6.86358434e-01\n",
      "   5.51066744e-02 -8.41542619e-01 -6.05800087e-01 -3.55243399e-01\n",
      "   7.61281964e-01  5.21600489e-01  5.95635217e-02 -1.62531069e-01\n",
      "   6.04000015e-01 -1.38733986e-01 -1.99591177e-01 -8.69820627e-01\n",
      "   2.08554761e-01  7.16914947e-01  8.01029079e-01 -2.10662354e-01\n",
      "  -8.23071313e-01 -5.24861957e-02 -3.05902150e-03  4.02117492e-02\n",
      "  -1.54121743e-02 -6.28441358e-01 -3.11308168e-01  6.03670906e-01\n",
      "   6.03782534e-01 -5.25527390e-01 -3.46000553e-01  9.10380429e-01\n",
      "  -4.24560535e-01  2.46894019e-01 -2.33105322e-01  7.92892858e-01\n",
      "   8.95919486e-02 -5.05729754e-01 -5.61483148e-01 -2.78832567e-01\n",
      "   7.11387181e-01  7.22749595e-01  5.15549634e-01 -2.18868386e-01\n",
      "  -5.92591854e-01 -1.36365080e-01 -3.84765108e-02 -1.38145512e-01\n",
      "  -4.65369912e-01  8.26597771e-01  9.61869375e-01 -7.42976208e-01\n",
      "   4.80089362e-01 -1.83881136e-01 -7.92015972e-01 -8.08642202e-01\n",
      "   8.68921636e-01  9.01059087e-01  8.32772831e-01  3.87319009e-01\n",
      "   6.92346919e-01 -3.16170499e-01 -7.48990135e-01 -2.68197271e-01\n",
      "   1.34551496e-01  6.30524456e-01 -2.92655688e-01 -5.81611492e-01\n",
      "  -7.82236248e-01  5.43651281e-01 -3.40572040e-01 -3.47838246e-01\n",
      "  -7.56465997e-01  1.77418144e-01  2.66178353e-01 -9.76467975e-01]]\n",
      "Layer_2:\n",
      "Pesos shape:(200, 2) bias shape:(1, 2) Activation:sigm\n",
      "Activation:sigm, Random:(0, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[0.45880008 0.15503718]\n",
      " [0.76204491 0.85722814]\n",
      " [0.51674971 0.49390761]\n",
      " [0.64361815 0.04213873]\n",
      " [0.94920586 0.51288028]\n",
      " [0.87587    0.60607004]\n",
      " [0.6319328  0.97418444]\n",
      " [0.04919364 0.30828973]\n",
      " [0.16516138 0.68428748]\n",
      " [0.28374809 0.26874698]\n",
      " [0.2974837  0.7722175 ]\n",
      " [0.04744601 0.46778085]\n",
      " [0.64941494 0.98152817]\n",
      " [0.57919536 0.88488025]\n",
      " [0.14515551 0.3374131 ]\n",
      " [0.47340081 0.81972547]\n",
      " [0.74031077 0.79768726]\n",
      " [0.19120527 0.84247637]\n",
      " [0.50132476 0.93078492]\n",
      " [0.65591897 0.52964937]\n",
      " [0.5050064  0.23780863]\n",
      " [0.09792118 0.45035788]\n",
      " [0.41479703 0.97440415]\n",
      " [0.63126624 0.48473025]\n",
      " [0.79473847 0.07791704]\n",
      " [0.17909907 0.77731662]\n",
      " [0.96749483 0.09000028]\n",
      " [0.77215676 0.74951065]\n",
      " [0.39514808 0.60186652]\n",
      " [0.01267581 0.42333049]\n",
      " [0.79325758 0.14988091]\n",
      " [0.1687856  0.35883182]\n",
      " [0.25237818 0.54860871]\n",
      " [0.42237133 0.28111039]\n",
      " [0.40065117 0.79306725]\n",
      " [0.52696845 0.72642282]\n",
      " [0.71552692 0.69898468]\n",
      " [0.36137579 0.79073949]\n",
      " [0.66245083 0.45879706]\n",
      " [0.35079807 0.61588851]\n",
      " [0.14870758 0.13914872]\n",
      " [0.44225188 0.79372583]\n",
      " [0.01802993 0.39052569]\n",
      " [0.56735647 0.49687973]\n",
      " [0.45128712 0.55668206]\n",
      " [0.05455548 0.35357002]\n",
      " [0.93361108 0.32384505]\n",
      " [0.22911138 0.18912856]\n",
      " [0.32745612 0.56555064]\n",
      " [0.72472743 0.59352175]\n",
      " [0.17459165 0.42549099]\n",
      " [0.10120556 0.93850575]\n",
      " [0.80731539 0.88697314]\n",
      " [0.03954413 0.71632383]\n",
      " [0.91390587 0.56345375]\n",
      " [0.62346613 0.08480216]\n",
      " [0.51862835 0.70949534]\n",
      " [0.22784025 0.37426485]\n",
      " [0.2561201  0.62026891]\n",
      " [0.87355869 0.18461368]\n",
      " [0.09208049 0.90402553]\n",
      " [0.38724147 0.18454391]\n",
      " [0.56991621 0.25565361]\n",
      " [0.70084236 0.09720481]\n",
      " [0.10068531 0.40445948]\n",
      " [0.90924132 0.80604808]\n",
      " [0.96722156 0.72846762]\n",
      " [0.24268412 0.38518344]\n",
      " [0.10804753 0.90724135]\n",
      " [0.58664196 0.03321352]\n",
      " [0.92729052 0.12009285]\n",
      " [0.5303341  0.10786233]\n",
      " [0.7822154  0.6550245 ]\n",
      " [0.26439096 0.41720459]\n",
      " [0.39024226 0.00826735]\n",
      " [0.84071889 0.62727632]\n",
      " [0.73614417 0.53354285]\n",
      " [0.14979693 0.41063472]\n",
      " [0.38779376 0.34710278]\n",
      " [0.72054655 0.48321897]\n",
      " [0.20883525 0.17217514]\n",
      " [0.67142276 0.99233327]\n",
      " [0.41709734 0.82464463]\n",
      " [0.10125392 0.17997324]\n",
      " [0.2248376  0.71745062]\n",
      " [0.82921611 0.34143226]\n",
      " [0.94411684 0.36288237]\n",
      " [0.71439209 0.58381579]\n",
      " [0.32067787 0.40778256]\n",
      " [0.2110609  0.27407447]\n",
      " [0.58035092 0.84120046]\n",
      " [0.88053962 0.66179425]\n",
      " [0.59861061 0.51790841]\n",
      " [0.2888952  0.39024457]\n",
      " [0.05587209 0.78872809]\n",
      " [0.89156351 0.22511047]\n",
      " [0.76442475 0.80678199]\n",
      " [0.07321485 0.92116998]\n",
      " [0.22979661 0.0530557 ]\n",
      " [0.40110507 0.40039106]\n",
      " [0.50111066 0.70050916]\n",
      " [0.38629771 0.78255278]\n",
      " [0.31719985 0.68298242]\n",
      " [0.22885548 0.98540578]\n",
      " [0.33188484 0.06152113]\n",
      " [0.56160586 0.94533083]\n",
      " [0.01163107 0.16349188]\n",
      " [0.27058935 0.02066867]\n",
      " [0.36601702 0.17016152]\n",
      " [0.7657073  0.59352694]\n",
      " [0.71968948 0.64471734]\n",
      " [0.37306286 0.59753012]\n",
      " [0.60058329 0.40616961]\n",
      " [0.04888245 0.3981554 ]\n",
      " [0.34008723 0.7945186 ]\n",
      " [0.71672935 0.10285194]\n",
      " [0.73656937 0.38106387]\n",
      " [0.59003262 0.9944024 ]\n",
      " [0.65939653 0.32537873]\n",
      " [0.34633177 0.22193554]\n",
      " [0.21743144 0.62011934]\n",
      " [0.79842018 0.96542382]\n",
      " [0.26691529 0.61880036]\n",
      " [0.04160244 0.12669101]\n",
      " [0.87969213 0.77913963]\n",
      " [0.86993445 0.70664178]\n",
      " [0.47764081 0.15808551]\n",
      " [0.66707602 0.37347363]\n",
      " [0.12393392 0.5920185 ]\n",
      " [0.32838505 0.11440768]\n",
      " [0.5635114  0.18646304]\n",
      " [0.87618737 0.11418428]\n",
      " [0.72222119 0.8508742 ]\n",
      " [0.03369406 0.96125701]\n",
      " [0.0333575  0.63104886]\n",
      " [0.01114857 0.09087535]\n",
      " [0.54927303 0.51699164]\n",
      " [0.01748818 0.24409719]\n",
      " [0.51489605 0.50400411]\n",
      " [0.89686758 0.85591832]\n",
      " [0.81976735 0.45761776]\n",
      " [0.82952695 0.09929055]\n",
      " [0.98048963 0.78330113]\n",
      " [0.58309391 0.32875013]\n",
      " [0.26079632 0.52942072]\n",
      " [0.21140454 0.51307778]\n",
      " [0.14928494 0.17824597]\n",
      " [0.01368699 0.08220412]\n",
      " [0.99992917 0.58257564]\n",
      " [0.63916722 0.2706275 ]\n",
      " [0.70888749 0.26320797]\n",
      " [0.62783015 0.36676675]\n",
      " [0.44984551 0.18125876]\n",
      " [0.36358287 0.26841989]\n",
      " [0.26957234 0.72964397]\n",
      " [0.74342396 0.29940413]\n",
      " [0.56101445 0.1054148 ]\n",
      " [0.69352131 0.81248576]\n",
      " [0.41363178 0.69663091]\n",
      " [0.69912983 0.01014922]\n",
      " [0.04248065 0.71671403]\n",
      " [0.90394564 0.49860491]\n",
      " [0.25369765 0.26194753]\n",
      " [0.56719182 0.81562426]\n",
      " [0.52360831 0.87865595]\n",
      " [0.08928564 0.08156506]\n",
      " [0.38732402 0.37134078]\n",
      " [0.72986646 0.87846263]\n",
      " [0.8628739  0.25911183]\n",
      " [0.64060246 0.08700023]\n",
      " [0.2615375  0.33705324]\n",
      " [0.20965828 0.04250882]\n",
      " [0.79347651 0.65634647]\n",
      " [0.05437369 0.95962006]\n",
      " [0.24706422 0.11466106]\n",
      " [0.86993774 0.46806833]\n",
      " [0.14475691 0.94611587]\n",
      " [0.47060194 0.44545772]\n",
      " [0.05202407 0.12854014]\n",
      " [0.24795995 0.79393813]\n",
      " [0.07238736 0.9813212 ]\n",
      " [0.24297902 0.05157765]\n",
      " [0.78726596 0.24217078]\n",
      " [0.46665556 0.75376928]\n",
      " [0.4651561  0.38511753]\n",
      " [0.56141229 0.15505925]\n",
      " [0.78403813 0.3040596 ]\n",
      " [0.24887931 0.16320137]\n",
      " [0.17784315 0.54371581]\n",
      " [0.96251629 0.70992785]\n",
      " [0.08080974 0.79462016]\n",
      " [0.30513245 0.07014677]\n",
      " [0.49397088 0.27926417]\n",
      " [0.10736278 0.54456574]\n",
      " [0.21270786 0.36212931]\n",
      " [0.02596069 0.81724843]\n",
      " [0.28485449 0.95987714]\n",
      " [0.94797925 0.12342419]\n",
      " [0.90937978 0.8150145 ]\n",
      " [0.58184544 0.72167764]]\n",
      "#####\n",
      "Bias:\n",
      "[[0.88260998 0.02559019]]\n"
     ]
    }
   ],
   "source": [
    "cnn=Model2()\n",
    "cnn.Show_Model()\n",
    "cnn.Show_Model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=999, Accuracy=98.75 Loss=0.0095336\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "\n",
    "\n",
    "loss,accuracy=cnn.Train(Xtrain,Ytrain,0.02,1000)\n",
    "Show_Loss_Acc(loss,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.53348059 0.26190476 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.13032545 0.64285714 0.05185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.98]]\n",
      "Input:[0.72563338 0.66666667 0.75555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.55642595 0.33333333 0.02222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.47520992 0.0952381  0.48888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.43022044 0.16666667 0.53333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.4181342  0.71428571 0.13333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.36401164 0.4047619  0.31851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.1041292  0.69047619 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 1.  ]]\n",
      "Input:[0.71834703 0.19047619 0.12592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.12935582 0.33333333 0.88888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.16006228 1.         0.14074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.38849795 0.80952381 0.04444444 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.98215227 0.47619048 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.41815029 0.16666667 0.55555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.68280044 0.95238095 0.17037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.12 0.87]]\n",
      "Input:[0.79628803 0.73809524 0.54074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.97]]\n",
      "Input:[0.59233465 0.45238095 0.46666667 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.2  0.79]]\n",
      "Input:[0.98366908 0.4047619  0.45925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.06020994 0.38095238 0.20740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.57448692 0.04761905 0.25185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.42068502 0.66666667 0.54074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.97]]\n",
      "Input:[0.42920655 0.30952381 0.14074074 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.26893103 0.69047619 0.11111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.46501869 0.4047619  0.25925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.22489107 0.5        0.6        0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.86841121 0.35714286 0.72592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.01]]\n",
      "Input:[0.19110269 0.73809524 0.52592593 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.43895923 0.64285714 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.89 0.08]]\n",
      "Input:[0.97923532 0.61904762 0.17777778 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.54425521 0.54761905 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.93956475 0.57142857 0.28148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.03623057 0.4047619  0.42962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.08170286 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.02]]\n",
      "Input:[0.74090615 0.21428571 0.6        1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.94]]\n",
      "Input:[0.28058677 0.28571429 0.74814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.43308509 0.54761905 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.65983094 0.54761905 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.03]]\n",
      "Input:[0.63453995 0.04761905 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.50362306 0.66666667 0.19259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.06 0.94]]\n",
      "Input:[0.86952166 0.21428571 0.11851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.19367765 0.4047619  0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.56707584 0.73809524 0.0962963  0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.38814389 0.4047619  0.56296296 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.09893501 0.45238095 0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.89488105 0.33333333 0.75555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.98542328 0.42857143 0.44444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.72199624 0.04761905 0.52592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.34484826 0.47619048 0.25925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.1100476  0.73809524 0.15555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.08243109 0.52380952 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.06]]\n",
      "Input:[0.7800577  0.45238095 0.45925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.86 0.09]]\n",
      "Input:[0.18001022 1.         0.22962963 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.96]]\n",
      "Input:[0.78560997 0.71428571 0.91111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.02911723 0.52380952 0.41481481 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.43 0.53]]\n",
      "Input:[0.6401405  0.42857143 0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.81323452 0.19047619 0.48888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.94371688 0.35714286 0.11851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.44852281 0.54761905 0.26666667 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.12188842 0.66666667 0.43703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.25058842 0.         0.4962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.13214402 0.45238095 0.13333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.03064209 0.4047619  0.28148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.1758782  0.23809524 0.32592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.37623065 0.45238095 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.79464246 0.         0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.21309451 0.47619048 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.64624397 0.4047619  0.37037037 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.99831018 0.30952381 0.37777778 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.67268565 0.69047619 0.07407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.35312034 0.57142857 0.65925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.23263608 0.02380952 0.02962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.94537049 0.07142857 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.59735583 0.23809524 0.12592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.6807123  0.80952381 1.         0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.08606421 0.83333333 0.65925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.91]]\n",
      "Input:[0.76542062 0.16666667 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.42276511 0.19047619 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.45448949 0.19047619 0.         1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.94228053 0.52380952 0.94074074 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.76906179 0.4047619  0.17037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.84048892 0.92857143 0.79259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "\u001b[31m Input:[0.04472796 0.54761905 0.53333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 0.]] predict_float:[[0.34 0.37]]\u001b[0m\n",
      "Input:[0.72313486 0.14285714 0.54814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.63039988 0.45238095 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.82925563 0.4047619  0.68888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.01]]\n",
      "Input:[0.71451275 0.33333333 0.62962963 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.07 0.86]]\n",
      "Input:[0.91557734 0.4047619  0.97777778 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.38374231 0.02380952 0.08148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.20594898 0.66666667 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.01]]\n",
      "Input:[0.6566927  0.69047619 0.66666667 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.73185353 0.64285714 0.22222222 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.14 0.81]]\n",
      "Input:[0.50925982 0.47619048 0.26666667 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.67380415 0.45238095 0.57777778 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.85654625 0.26190476 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.73403018 0.54761905 0.11111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.12625781 0.52380952 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.80306743 0.21428571 0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.63637461 0.21428571 0.55555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.39721662 0.5        0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.19204014 0.11904762 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.6251212  0.78571429 0.97037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.04553264 0.11904762 0.35555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.91411685 0.45238095 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.15545551 0.52380952 0.23703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.21884794 0.71428571 0.1037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.2029596  0.4047619  0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.78824528 0.19047619 0.11111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.09157222 0.5        0.67407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.59888874 0.23809524 0.51111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.48877275 0.97619048 0.45185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.96]]\n",
      "Input:[0.37760263 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.75001106 0.4047619  0.60740741 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.94]]\n",
      "Input:[0.62062306 0.19047619 0.52592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.60387774 0.54761905 0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.05267414 0.26190476 0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.35580393 0.19047619 0.01481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.73504005 0.95238095 0.05925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.93330436 0.28571429 0.34814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.13633639 0.16666667 0.13333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.96897569 0.97619048 0.1037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.76497001 0.23809524 0.16296296 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.05038484 0.5        0.45925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.03]]\n",
      "Input:[0.29325238 0.69047619 0.68148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.95]]\n",
      "Input:[0.83760013 0.80952381 0.91111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.71551859 0.26190476 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.65325673 0.21428571 0.9037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.33171593 0.14285714 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.50340177 0.52380952 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.02721015 0.07142857 0.00740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.50746941 0.92857143 0.08148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.08 0.96]]\n",
      "Input:[0.64779297 0.28571429 0.88888889 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.09264244 0.71428571 0.77037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.96]]\n",
      "Input:[0.91654295 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.84553425 0.4047619  0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.03]]\n",
      "Input:[0.84508765 0.23809524 0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.05871324 0.45238095 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.58924469 0.4047619  0.08888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.28527401 0.45238095 0.47407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.14 0.77]]\n",
      "Input:[0.06836534 0.33333333 0.77777778 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.14357043 0.26190476 0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.6279255  0.28571429 0.01481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.93407283 0.16666667 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.99145836 0.52380952 0.68148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.98]]\n",
      "Input:[0.45492804 0.14285714 0.02962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.17279629 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.62421192 0.47619048 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.45825538 0.42857143 0.95555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.47101353 0.57142857 0.36296296 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97023098 0.71428571 0.13333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.21233811 0.71428571 0.11111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.29204939 0.11904762 0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.35221105 0.88095238 0.85185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.98]]\n",
      "Input:[0.89772156 0.26190476 0.98518519 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.81760391 0.57142857 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.03]]\n",
      "Input:[0.84353865 0.21428571 0.28888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.83186681 0.45238095 0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.11220413 0.16666667 0.05185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.6690807  0.97619048 0.94814815 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.88860859 0.57142857 0.28888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.7955437  0.21428571 0.01481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.68426092 0.69047619 0.25185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.29 0.64]]\n",
      "Input:[0.10591558 0.23809524 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.5266006  0.35714286 0.4        1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.1152096  0.30952381 0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.42231852 0.4047619  0.05925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.56946573 0.4047619  0.05185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.9664651  0.92857143 0.13333333 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.48603282 0.28571429 0.68148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.55693289 0.66666667 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.51897629 0.54761905 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.01758219 0.35714286 0.19259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.77400653 0.35714286 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.80435491 0.69047619 0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.37 0.57]]\u001b[0m\n",
      "Input:[0.93894515 0.23809524 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.70616423 0.64285714 0.12592593 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.45306522 0.4047619  0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.56692296 0.97619048 0.2        1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.86 0.07]]\n",
      "Input:[0.81024112 0.69047619 0.25925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.41 0.55]]\n",
      "Input:[0.25367436 0.69047619 0.26666667 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.98]]\n",
      "Input:[0.18417442 0.4047619  0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.51404362 0.5        0.2        0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.06186757 0.52380952 0.46666667 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.87 0.07]]\n",
      "Input:[0.84037627 0.11904762 0.24444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.11294846 0.5        0.44444444 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.12 0.84]]\n",
      "Input:[0.         0.4047619  0.31111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.23619678 0.88095238 0.17777778 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.83701674 0.30952381 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.06358154 0.23809524 0.8        0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.69391705 0.57142857 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.02]]\n",
      "Input:[0.30592202 0.52380952 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.44865559 0.71428571 0.43703704 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.08 0.9 ]]\n",
      "Input:[0.08177125 0.07142857 0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.32286449 0.83333333 0.42222222 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.35491074 0.21428571 0.31851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.17286067 0.4047619  0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.49512567 0.23809524 0.54814815 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.71774755 0.19047619 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.51337976 0.33333333 0.75555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.98]]\n",
      "Input:[0.87903294 0.57142857 0.44444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.43959493 0.35714286 0.99259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.8961806  0.54761905 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.43652106 0.30952381 0.31851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.08074127 0.5        0.88148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.99504319 0.21428571 0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.88636757 0.26190476 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.38709781 0.02380952 0.40740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97052067 0.02380952 0.51851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.85767682 0.26190476 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.67938861 0.5        0.88148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.53333172 0.30952381 0.45185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.59052815 0.54761905 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.01641541 0.19047619 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.06617662 0.52380952 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.94 0.04]]\n",
      "Input:[0.57762918 0.4047619  0.07407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.63507908 0.71428571 0.6        1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.54254527 0.57142857 0.99259259 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.96879866 0.23809524 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.14988312 0.78571429 0.88148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.03]]\n",
      "Input:[0.49688389 0.35714286 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.57330807 0.57142857 0.28888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "Input:[0.33312814 0.64285714 0.05185185 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.73887836 0.45238095 0.31111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.34317051 0.38095238 0.71851852 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.24878192 0.4047619  0.17777778 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.83481193 0.0952381  0.08888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.80866798 0.4047619  0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.76823699 0.5        0.32592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.80943242 0.04761905 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.31946473 0.66666667 0.12592593 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.40992649 0.19047619 0.20740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.71708771 0.26190476 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.42700978 0.88095238 0.81481481 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.07093628 0.45238095 0.97037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[1.         0.64285714 0.85925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.14737655 0.35714286 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.03834285 0.54761905 0.22222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.81113029 0.57142857 0.47407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.61982643 0.45238095 0.9037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.65886533 0.14285714 0.51111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.10536438 0.33333333 0.02222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.81637276 0.9047619  0.65925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "\u001b[31m Input:[0.63352605 0.73809524 0.17777778 1.         0.        ]-- Real:[0. 1.] predict: [[0. 0.]] predict_float:[[0.49 0.45]]\u001b[0m\n",
      "Input:[0.26241717 0.23809524 0.51851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.82633868 0.83333333 0.94814815 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.87041887 0.28571429 0.54814815 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.99725203 0.92857143 0.33333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.62323424 0.52380952 0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.33385637 0.54761905 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.91 0.04]]\n",
      "Input:[0.40964083 0.04761905 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.78672847 0.0952381  0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.58149565 0.4047619  0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.92 0.07]]\n",
      "Input:[0.85166588 0.85714286 0.40740741 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.17482005 0.30952381 0.         1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.77649298 0.76190476 0.15555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.97]]\n",
      "Input:[0.17747951 0.57142857 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.40657501 0.38095238 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.94407496 0.57142857 0.68888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.97]]\n",
      "Input:[0.23362181 0.85714286 0.08148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.42407271 0.02380952 0.04444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.21081727 0.42857143 0.25925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.73932898 0.45238095 0.40740741 0.         1.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.69 0.26]]\u001b[0m\n",
      "Input:[0.91948404 0.42857143 0.82222222 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.89]]\n",
      "Input:[0.25938756 0.69047619 0.03703704 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.6910524  0.28571429 0.47407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.15413986 0.97619048 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.83 0.03]]\n",
      "Input:[0.52705122 0.26190476 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.55767722 0.5952381  0.71851852 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.52830249 0.47619048 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.01]]\n",
      "Input:[0.39516872 0.95238095 0.95555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.1209912  0.42857143 0.81481481 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.90980378 0.73809524 0.0962963  0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.72440223 0.         0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.90654484 0.28571429 0.         0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.17292504 0.97619048 0.54074074 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.16 0.81]]\n",
      "Input:[0.14708285 0.21428571 0.31111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.85720206 0.47619048 0.41481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.01]]\n",
      "Input:[0.91380705 0.30952381 0.54814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.4250343  0.69047619 0.14074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.05837528 0.30952381 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.70151722 0.45238095 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.89 0.08]]\n",
      "Input:[0.49305765 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.5717309  0.97619048 0.85185185 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.2789573  0.73809524 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.03]]\n",
      "Input:[0.1355478  0.71428571 0.55555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.5259971  0.66666667 0.05185185 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.40526741 0.85714286 0.65925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.98273164 0.4047619  0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.75306481 0.73809524 0.93333333 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.59533609 0.71428571 0.19259259 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.47545132 0.4047619  0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.22858051 0.42857143 0.33333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.51555641 0.33333333 1.         1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.40013358 0.         0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.51602715 0.14285714 0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.08340475 0.57142857 0.55555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.68212048 0.47619048 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.04]]\n",
      "Input:[0.85633703 1.         0.68888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.04220127 0.0952381  0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.51381027 0.14285714 0.12592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.90318934 0.66666667 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.02]]\n",
      "Input:[0.91978177 0.71428571 0.88148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.42124025 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.63357031 0.76190476 0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.9  0.07]]\n",
      "Input:[0.67441168 0.47619048 0.41481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.94 0.03]]\n",
      "Input:[0.07010746 0.14285714 0.05925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.47816711 0.4047619  0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.75311712 0.45238095 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.03]]\n",
      "Input:[0.64240969 0.14285714 0.08888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.90692304 0.19047619 0.51111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.22588887 0.38095238 0.07407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.14025919 0.42857143 0.28888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.29078605 0.07142857 0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97630629 0.54761905 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.27843426 1.         0.2        0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.73775584 0.80952381 0.55555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.11482335 0.04761905 0.05925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.5607873  0.78571429 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.32001191 0.66666667 0.47407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.96]]\n",
      "Input:[0.49381405 0.28571429 0.25185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "::::::::::::::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_predict(cnn,Xtrain,Ytrain)\n",
    "print(\"::::::::::::::::::::::::::::\")\n",
    "\n",
    "\n",
    "np.round(cnn.Predict(np.array([0.51,0.35,0.14,0.02,0.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
