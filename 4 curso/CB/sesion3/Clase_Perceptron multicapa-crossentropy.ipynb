{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FUNCIONES DE ACTIVACIÓN PARA REDES NEURONALES MULTICAPA\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit,softmax\n",
    "\n",
    "class function(object):\n",
    "    def __init__(self,funcion,derivative=None,rand_init=(0,1)):\n",
    "        self.F=funcion\n",
    "        self.D=derivative\n",
    "        self.Rand_init=rand_init\n",
    "\n",
    "lineal=function(funcion=lambda x:x,\n",
    "                derivative=lambda x:1,\n",
    "                rand_init=(-1,1))\n",
    "\n",
    "sigm=function(funcion=lambda x: expit(x),\n",
    "              derivative=lambda x: expit(x)*(1-expit(x)),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "tanh=function(funcion=lambda x:(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)),\n",
    "              derivative=lambda x:1-((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))**2,\n",
    "              rand_init=(-1,1))\n",
    "\n",
    "tanh1=function(funcion=lambda x:np.tanh(x),\n",
    "               derivative=lambda x:1-np.tanh(x)**2,\n",
    "               rand_init=(-1,1))\n",
    "\n",
    "relu=function(funcion=lambda x: np.maximum(0, x),\n",
    "              derivative=lambda x: np.where(x<=0,0,1),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "softmaxf=function(funcion=lambda x: softmax(x),\n",
    "                  derivative=lambda x:softmax(x)*(1-softmax(x)),\n",
    "                  rand_init=(0,1))\n",
    "\n",
    "\n",
    "# funciones de coste\n",
    "mse=function(funcion=lambda Yp, Yr: np.mean((Yp - Yr) ** 2) ,\n",
    "                 derivative=lambda Yp, Yr: (Yp - Yr))\n",
    "\n",
    "cross_entropy=function(funcion=lambda yscore,yreal:-np.sum(yreal*np.log(yscore))/yscore.shape[0],\n",
    "                       derivative=lambda yscore,yreal:yscore-yreal)\n",
    "\n",
    "Funciones={\"relu\":relu,\n",
    "           \"sigm\":sigm,\n",
    "           \"relu\":relu,\n",
    "           \"tanh\":tanh,\n",
    "           \"tanh1\":tanh1,\n",
    "           \"lineal\":lineal,\n",
    "           \"softmax\":softmaxf}\n",
    "\n",
    "Loss={\"mse\":mse,\n",
    "      \"cross_entropy\":cross_entropy}\n",
    "\n",
    "_x = np.linspace(-10, 10, 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(_x, softmaxf.F(_x),\"tab:blue\")\n",
    "plt.plot(_x, softmaxf.D(_x),\"tab:red\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "Min_Max = preprocessing.MinMaxScaler()\n",
    "Ordinal =preprocessing.OrdinalEncoder()\n",
    "\n",
    "\n",
    "#def one_hot_cols(df,cols_to_one):\n",
    "#    one_hot=pd.get_dummies(df,cols_to_one,columns=cols_to_one)\n",
    "#    return one_hot\n",
    "\n",
    "def fit_cols(df, cols_to_fit,fit_function ):\n",
    "    for col in cols_to_fit:\n",
    "        df[col] = pd.DataFrame(fit_function.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  Fjob  ...  \\\n",
       "0       0.0  0.0   18      1.0      0.0      0.0     4     4   0.0   4.0  ...   \n",
       "1       0.0  0.0   17      1.0      0.0      1.0     1     1   0.0   2.0  ...   \n",
       "2       0.0  0.0   15      1.0      1.0      1.0     1     1   0.0   2.0  ...   \n",
       "3       0.0  0.0   15      1.0      0.0      1.0     4     2   1.0   3.0  ...   \n",
       "4       0.0  0.0   16      1.0      0.0      1.0     3     3   2.0   2.0  ...   \n",
       "..      ...  ...  ...      ...      ...      ...   ...   ...   ...   ...  ...   \n",
       "390     1.0  1.0   20      1.0      1.0      0.0     2     2   3.0   3.0  ...   \n",
       "391     1.0  1.0   17      1.0      1.0      1.0     3     1   3.0   3.0  ...   \n",
       "392     1.0  1.0   21      0.0      0.0      1.0     1     1   2.0   2.0  ...   \n",
       "393     1.0  1.0   18      0.0      1.0      1.0     3     2   3.0   2.0  ...   \n",
       "394     1.0  1.0   19      1.0      1.0      1.0     1     1   2.0   0.0  ...   \n",
       "\n",
       "     famrel  freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0         4         3      4     1     1      3        6   5   6   6  \n",
       "1         5         3      3     1     1      3        4   5   5   6  \n",
       "2         4         3      2     2     3      3       10   7   8  10  \n",
       "3         3         2      2     1     1      5        2  15  14  15  \n",
       "4         4         3      2     1     2      5        4   6  10  10  \n",
       "..      ...       ...    ...   ...   ...    ...      ...  ..  ..  ..  \n",
       "390       5         5      4     4     5      4       11   9   9   9  \n",
       "391       2         4      5     3     4      2        3  14  16  16  \n",
       "392       5         5      3     3     3      3        3  10   8   7  \n",
       "393       4         4      1     3     4      5        0  11  12  10  \n",
       "394       3         2      3     3     3      5        5   8   9   9  \n",
       "\n",
       "[395 rows x 33 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hacer aquí la normalizacion de los datos\n",
    "import csv\n",
    "estudiantes = pd.read_csv('datasets/Maths.csv', encoding='utf-8' )\n",
    "dataset1=fit_cols(estudiantes,['sex','school','famsize','Pstatus','Mjob','Fjob','address','reason','guardian','schoolsup','famsup','activities','nursery','higer',],Ordinal)\n",
    "dataset1\n",
    "#X=[]\n",
    "#Y=[]\n",
    "#with open('datasets/iris.csv') as csvfile:\n",
    "#    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "#    for row in readCSV:\n",
    "#        row=row[0].split(\",\")\n",
    "#        X.append([float(row[0])/10,float(row[1])/10,float(row[2])/10,float(row[3])/10])\n",
    "#\n",
    "#        if row[4]=='\"Setosa\"':\n",
    "#            Y.append([1,0,0])\n",
    "#        if row[4]=='\"Versicolor\"':\n",
    "#            Y.append([0,1,0])\n",
    "#        if row[4]=='\"Virginica\"':\n",
    "#            Y.append([0,0,1])\n",
    "#\n",
    "#\n",
    "#Xtrain=np.array(X)\n",
    "#Ytrain=np.array(Y)\n",
    "#Xtrain=Xtrain[1:-1]\n",
    "#Ytrain=Ytrain[1:-1]\n",
    "#print(np.shape(Xtrain))\n",
    "#print(Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE DE LA CAPA DE LA RED\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class neural_layer(object):\n",
    "    def __init__(self, n_conn, n_neur, activation=\"relu\"):\n",
    "        self.act = Funciones[activation]\n",
    "        self.activation=activation\n",
    "        self.random=self.act.Rand_init  \n",
    "        self.shape=(n_conn,n_neur)\n",
    "        self.Initialize()\n",
    "        \n",
    "    def show(self,Full=False):\n",
    "        print(f\"Pesos shape:{np.shape(self.W)} bias shape:{np.shape(self.b)} Activation:{self.activation}\")\n",
    "        print(f\"Activation:{self.activation}, Random:{self.random}\")\n",
    "        print(\"______________________\")\n",
    "        if Full:\n",
    "            print(f\"Pesos:\")\n",
    "            print(self.W)\n",
    "            print(\"#####\")\n",
    "            print(f\"Bias:\")\n",
    "            print(self.b)\n",
    "            \n",
    "    def Initialize(self):\n",
    "        #inicializa los pesos iniciales con aleatorios\n",
    "        self.b = np.random.uniform(*self.random,(1, self.shape[1]))      \n",
    "        self.W = np.random.uniform(*self.random,self.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "#CLASE RED NEURONAL MULTICAPA        \n",
    "class Neural_Net(object):\n",
    "    def __init__(self,Input,loss):\n",
    "        self.loss = Loss[loss]\n",
    "        self.Funcion_Loss=loss\n",
    "        self.Input=Input\n",
    "        self.NN=None;\n",
    "              \n",
    "    def Add_Layer(self,Num_neurons, function):\n",
    "        if self.NN is None:\n",
    "            self.NN=[]\n",
    "            self.NN.append(neural_layer(self.Input,Num_neurons,function))\n",
    "        else:\n",
    "            _,L_input=np.shape(self.NN[-1].W)\n",
    "            self.NN.append(neural_layer(L_input,Num_neurons,function))\n",
    "            \n",
    "    def Show_Model(self, Full=False):\n",
    "        print(f\"Input shape:{self.Input}, Loss: {self.Funcion_Loss}\")\n",
    "        for i,L in enumerate(self.NN):\n",
    "            print(F\"Layer_{i}:\")\n",
    "            L.show(Full)\n",
    "            \n",
    "            \n",
    "    # fucnción de predicción (fordware pass)    \n",
    "    def Predict(self,X):  \n",
    "      #sólo podemos pasar Numpy  \n",
    "      sx=np.shape(X)\n",
    "      X=X.reshape(1,sx[0])\n",
    "      if self.NN is None:\n",
    "          print(\"error in Predict Method ( not NEURAL network available)\")\n",
    "          return 0\n",
    "        \n",
    "      out = [(None, X)] #primer data necesario\n",
    "      # Forward pass\n",
    "      for l, layer in enumerate(self.NN):\n",
    "          z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "          a = self.NN[l].act.F(z)\n",
    "          out.append((z, a))\n",
    "      return out[-1][1]\n",
    "    \n",
    "    \n",
    "    # función retropropagación del error\n",
    "    def _backward_pass(self, X, Y,lr=0.01):\n",
    "      sx=np.shape(X)\n",
    "      sy=np.shape(Y)   \n",
    "      X=X.reshape(1,sx[0])\n",
    "      Y=Y.reshape(1,sy[0])\n",
    "\n",
    "      # Forward pass\n",
    "      out = [(None, X)] #primer data necesario\n",
    "      for l, layer in enumerate(self.NN):\n",
    "            z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "            a = self.NN[l].act.F(z)\n",
    "            out.append((z, a))\n",
    "\n",
    "      # Backward pass \n",
    "      deltas = []\n",
    "      for l in reversed(range(0, len(self.NN))):\n",
    "        z = out[l+1][0]\n",
    "        a = out[l+1][1]\n",
    "        if l == len(self.NN) - 1:\n",
    "            deltas.insert(0, self.loss.D(a, Y) * self.NN[l].act.D(a)) # La última capa\n",
    "        else:\n",
    "            deltas.insert(0, deltas[0] @ _W.T * self.NN[l-1].act.D(a))\n",
    "        _W = self.NN[l].W #los pesos en la capa superior\n",
    " \n",
    "        # Gradient descent. actualizamos pesos \n",
    "        self.NN[l].b = self.NN[l].b - (deltas[0]* lr)\n",
    "        self.NN[l].W = self.NN[l].W - (lr * (out[l][1].T @ deltas[0]))\n",
    "      return out[-1][1]\n",
    "\n",
    "    # función de entrenamiento de la red\n",
    "    def Train(self,X,Y,lr=0.01,epoch=10,batch_size=1):\n",
    "        H_loss = []\n",
    "        H_acc=[]\n",
    "        \n",
    "        # inicializamos las capas neuronales a valores ramdom del rango de la función\n",
    "        for Layer in self.NN:\n",
    "            Layer.Initialize()\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            account=0\n",
    "            epoch_Loss=0\n",
    "            epoch_Acc=0\n",
    "            # Entrenemos a la red! con el dataset de validación\n",
    "            for j in range(len(X)):\n",
    "                pY = self._backward_pass(X[j,:], Y[j,:],lr)#fila, fila, learning rate\n",
    "                epoch_Loss+=self.loss.F(pY[0],Y[j,:])\n",
    "                if (Y[j,:]==np.round(pY)).all():#condicion de acertar\n",
    "                    epoch_Acc+=1\n",
    "            H_acc.append(epoch_Acc/len(Y)*100)    \n",
    "            H_loss.append(epoch_Loss/len(Y))#media del error\n",
    "            \n",
    "            #imprimimos por pantalla resultados\n",
    "            print(\"Epoch={}, Accurary={} Loss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7)))\n",
    "            clear_output(wait=True)\n",
    "        print(\"Epoch={}, Accuracy={} Loss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7)))\n",
    "        return H_loss,H_acc\n",
    "\n",
    "    \n",
    "# VISUALIZACIÓN Y TEST\n",
    "def Show_Loss_Acc(H_loss,H_acc):\n",
    "    plt.plot(range(len(H_loss)), H_loss,\"tab:blue\")\n",
    "    plt.ylabel(\"loss function \")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "    plt.plot(range(len(H_acc)), H_acc, \"tab:red\")\n",
    "    plt.ylabel(\"ACCURACY\")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "       \n",
    "def print_predict(neural_net,X,Y):\n",
    "    for i in range(len(X)):\n",
    "        sal_float=neural_net.Predict(X[i])\n",
    "        sal=np.round(sal_float)\n",
    "        \n",
    "        if (Y[i]==np.round(sal)).all():\n",
    "            print(\"Input:{}-- Real:{} predict: {} predict_float:{}\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "        else:\n",
    "            print(\"\\x1b[31m Input:{}-- Real:{} predict: {} predict_float:{}\\x1b[0m\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINIMOS LOS MODELOs\n",
    "def Model1():\n",
    "    red=Neural_Net(Input=4,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(18,\"relu\")\n",
    "    red.Add_Layer(3,\"softmax\")\n",
    "    return red\n",
    "\n",
    "def Model2():\n",
    "    red=Neural_Net(Input=4,loss=\"mse\")\n",
    "    red.Add_Layer(8,\"tanh\")\n",
    "    red.Add_Layer(3,\"sigm\")\n",
    "    return red\n",
    "\n",
    "def Model3(): \n",
    "    red=Neural_Net(Input=4,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(16,\"sigm\")\n",
    "    red.Add_Layer(24,\"tanh\")\n",
    "    red.Add_Layer(8,\"relu\")\n",
    "    red.Add_Layer(3,\"softmax\")\n",
    "    return red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:4, Loss: cross_entropy\n",
      "Layer_0:\n",
      "Pesos shape:(4, 18) bias shape:(1, 18) Activation:relu\n",
      "Activation:relu, Random:(0, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[0.86745721 0.05998943 0.17086237 0.48126342 0.87638024 0.8207876\n",
      "  0.21959767 0.92537419 0.13053948 0.50721893 0.84977806 0.68595942\n",
      "  0.85299717 0.1958458  0.1762356  0.20131533 0.76504768 0.22942774]\n",
      " [0.71629967 0.57214855 0.61254776 0.31879545 0.38218942 0.36851811\n",
      "  0.94292864 0.65838132 0.92006335 0.38006288 0.61230622 0.24607676\n",
      "  0.56877802 0.45300441 0.24013256 0.54096239 0.66366328 0.74538729]\n",
      " [0.55299074 0.20196951 0.38182351 0.68145303 0.33430667 0.78431682\n",
      "  0.55688807 0.41479605 0.85136663 0.85467814 0.79045953 0.58092643\n",
      "  0.08381972 0.49497188 0.6648235  0.30540845 0.65150755 0.93728587]\n",
      " [0.1762181  0.67842585 0.37066939 0.01365761 0.2583893  0.64327766\n",
      "  0.46766748 0.47341304 0.26455259 0.74993593 0.31991285 0.67428804\n",
      "  0.92497113 0.07683379 0.72259801 0.01560119 0.26252084 0.39976486]]\n",
      "#####\n",
      "Bias:\n",
      "[[0.82585347 0.62819882 0.32203586 0.81120709 0.53765906 0.11095215\n",
      "  0.80898343 0.12780569 0.41383184 0.04754482 0.13687133 0.66636161\n",
      "  0.33725498 0.28397664 0.50622727 0.9328935  0.90516324 0.39607346]]\n",
      "Layer_1:\n",
      "Pesos shape:(18, 3) bias shape:(1, 3) Activation:softmax\n",
      "Activation:softmax, Random:(0, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[0.2517732  0.3932871  0.16708707]\n",
      " [0.80534382 0.39551553 0.72455816]\n",
      " [0.74196038 0.99619443 0.44679137]\n",
      " [0.16611235 0.55389648 0.12154088]\n",
      " [0.01593411 0.12690581 0.8153753 ]\n",
      " [0.29108776 0.61871267 0.1932661 ]\n",
      " [0.85929645 0.32736997 0.68637354]\n",
      " [0.6210372  0.00771984 0.87535216]\n",
      " [0.98479704 0.91630547 0.27197512]\n",
      " [0.47014609 0.0890679  0.22032656]\n",
      " [0.4844769  0.08802825 0.19641361]\n",
      " [0.62625004 0.47598039 0.40912437]\n",
      " [0.74654688 0.68666051 0.02200134]\n",
      " [0.52774474 0.23784805 0.8052699 ]\n",
      " [0.9862548  0.2605802  0.89993845]\n",
      " [0.14394419 0.4675604  0.33099398]\n",
      " [0.21902261 0.00281254 0.42323991]\n",
      " [0.56054394 0.22767692 0.96809457]]\n",
      "#####\n",
      "Bias:\n",
      "[[0.14106093 0.50818193 0.7329766 ]]\n"
     ]
    }
   ],
   "source": [
    "cnn=Model1()\n",
    "#cnn.Show_Model()\n",
    "cnn.Show_Model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss,accuracy\u001b[38;5;241m=\u001b[39mcnn\u001b[38;5;241m.\u001b[39mTrain(\u001b[43mXtrain\u001b[49m,Ytrain,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m      2\u001b[0m Show_Loss_Acc(loss,accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xtrain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "loss,accuracy=cnn.Train(Xtrain,Ytrain,0.2,300)\n",
    "Show_Loss_Acc(loss,accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.49 0.3  0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.47 0.32 0.13 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.46 0.31 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.36 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.54 0.39 0.17 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.46 0.34 0.14 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.34 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.44 0.29 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.49 0.31 0.15 0.01]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.54 0.37 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.48 0.34 0.16 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.48 0.3  0.14 0.01]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.43 0.3  0.11 0.01]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[1. 0. 0.]]\n",
      "Input:[0.58 0.4  0.12 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.57 0.44 0.15 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.54 0.39 0.13 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.51 0.35 0.14 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.57 0.38 0.17 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.51 0.38 0.15 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.54 0.34 0.17 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.51 0.37 0.15 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.46 0.36 0.1  0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[1. 0. 0.]]\n",
      "Input:[0.51 0.33 0.17 0.05]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.97 0.03 0.  ]]\n",
      "Input:[0.48 0.34 0.19 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.5  0.3  0.16 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.5  0.34 0.16 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.52 0.35 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.52 0.34 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.47 0.32 0.16 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.48 0.31 0.16 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.54 0.34 0.15 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.52 0.41 0.15 0.01]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.55 0.42 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.49 0.31 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.32 0.12 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.55 0.35 0.13 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.49 0.36 0.14 0.01]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.44 0.3  0.13 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.51 0.34 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.35 0.13 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.45 0.23 0.13 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.44 0.32 0.13 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.35 0.16 0.06]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.98 0.02 0.  ]]\n",
      "Input:[0.51 0.38 0.19 0.04]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.97 0.03 0.  ]]\n",
      "Input:[0.48 0.3  0.14 0.03]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.51 0.38 0.16 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.46 0.32 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.53 0.37 0.15 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "Input:[0.5  0.33 0.14 0.02]-- Real:[1 0 0] predict: [[1. 0. 0.]] predict_float:[[0.99 0.01 0.  ]]\n",
      "\u001b[31m Input:[0.7  0.32 0.47 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.16 0.84]]\u001b[0m\n",
      "\u001b[31m Input:[0.64 0.32 0.45 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.16 0.84]]\u001b[0m\n",
      "\u001b[31m Input:[0.69 0.31 0.49 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.05 0.95]]\u001b[0m\n",
      "\u001b[31m Input:[0.55 0.23 0.4  0.13]-- Real:[0 1 0] predict: [[0. 0. 0.]] predict_float:[[0.02 0.48 0.5 ]]\u001b[0m\n",
      "\u001b[31m Input:[0.65 0.28 0.46 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.  0.1 0.9]]\u001b[0m\n",
      "\u001b[31m Input:[0.57 0.28 0.45 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.14 0.86]]\u001b[0m\n",
      "\u001b[31m Input:[0.63 0.33 0.47 0.16]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.05 0.95]]\u001b[0m\n",
      "Input:[0.49 0.24 0.33 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.28 0.7  0.02]]\n",
      "\u001b[31m Input:[0.66 0.29 0.46 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.  0.2 0.8]]\u001b[0m\n",
      "\u001b[31m Input:[0.52 0.27 0.39 0.14]-- Real:[0 1 0] predict: [[0. 0. 0.]] predict_float:[[0.03 0.48 0.48]]\u001b[0m\n",
      "Input:[0.5  0.2  0.35 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.16 0.78 0.06]]\n",
      "\u001b[31m Input:[0.59 0.3  0.42 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.29 0.7 ]]\u001b[0m\n",
      "Input:[0.6  0.22 0.4  0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.04 0.77 0.19]]\n",
      "\u001b[31m Input:[0.61 0.29 0.47 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.07 0.93]]\u001b[0m\n",
      "Input:[0.56 0.29 0.36 0.13]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.12 0.79 0.09]]\n",
      "\u001b[31m Input:[0.67 0.31 0.44 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.34 0.65]]\u001b[0m\n",
      "\u001b[31m Input:[0.56 0.3  0.45 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.08 0.92]]\u001b[0m\n",
      "Input:[0.58 0.27 0.41 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.04 0.7  0.26]]\n",
      "\u001b[31m Input:[0.62 0.22 0.45 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.09 0.91]]\u001b[0m\n",
      "Input:[0.56 0.25 0.39 0.11]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.06 0.74 0.21]]\n",
      "\u001b[31m Input:[0.59 0.32 0.48 0.18]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\u001b[0m\n",
      "Input:[0.61 0.28 0.4  0.13]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.03 0.66 0.3 ]]\n",
      "\u001b[31m Input:[0.63 0.25 0.49 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.02 0.98]]\u001b[0m\n",
      "\u001b[31m Input:[0.61 0.28 0.47 0.12]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.12 0.88]]\u001b[0m\n",
      "\u001b[31m Input:[0.64 0.29 0.43 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.44 0.55]]\u001b[0m\n",
      "\u001b[31m Input:[0.66 0.3  0.44 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.31 0.68]]\u001b[0m\n",
      "\u001b[31m Input:[0.68 0.28 0.48 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.08 0.92]]\u001b[0m\n",
      "\u001b[31m Input:[0.67 0.3  0.5  0.17]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\u001b[0m\n",
      "\u001b[31m Input:[0.6  0.29 0.45 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.  0.1 0.9]]\u001b[0m\n",
      "Input:[0.57 0.26 0.35 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.18 0.8  0.02]]\n",
      "Input:[0.55 0.24 0.38 0.11]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.07 0.77 0.16]]\n",
      "Input:[0.55 0.24 0.37 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.11 0.81 0.08]]\n",
      "Input:[0.58 0.27 0.39 0.12]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.05 0.73 0.21]]\n",
      "\u001b[31m Input:[0.6  0.27 0.51 0.16]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\u001b[0m\n",
      "\u001b[31m Input:[0.54 0.3  0.45 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.06 0.93]]\u001b[0m\n",
      "\u001b[31m Input:[0.6  0.34 0.45 0.16]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.09 0.9 ]]\u001b[0m\n",
      "\u001b[31m Input:[0.67 0.31 0.47 0.15]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.09 0.91]]\u001b[0m\n",
      "\u001b[31m Input:[0.63 0.23 0.44 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.25 0.75]]\u001b[0m\n",
      "\u001b[31m Input:[0.56 0.3  0.41 0.13]-- Real:[0 1 0] predict: [[0. 0. 0.]] predict_float:[[0.03 0.48 0.49]]\u001b[0m\n",
      "Input:[0.55 0.25 0.4  0.13]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.03 0.5  0.47]]\n",
      "\u001b[31m Input:[0.55 0.26 0.44 0.12]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.21 0.79]]\u001b[0m\n",
      "\u001b[31m Input:[0.61 0.3  0.46 0.14]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.   0.1  0.89]]\u001b[0m\n",
      "Input:[0.58 0.26 0.4  0.12]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.04 0.65 0.31]]\n",
      "Input:[0.5  0.23 0.33 0.1 ]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.26 0.72 0.02]]\n",
      "\u001b[31m Input:[0.56 0.27 0.42 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.34 0.65]]\u001b[0m\n",
      "\u001b[31m Input:[0.57 0.3  0.42 0.12]-- Real:[0 1 0] predict: [[0. 0. 0.]] predict_float:[[0.02 0.48 0.5 ]]\u001b[0m\n",
      "\u001b[31m Input:[0.57 0.29 0.42 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.02 0.39 0.6 ]]\u001b[0m\n",
      "\u001b[31m Input:[0.62 0.29 0.43 0.13]-- Real:[0 1 0] predict: [[0. 0. 1.]] predict_float:[[0.01 0.39 0.6 ]]\u001b[0m\n",
      "Input:[0.51 0.25 0.3  0.11]-- Real:[0 1 0] predict: [[0. 1. 0.]] predict_float:[[0.41 0.59 0.  ]]\n",
      "\u001b[31m Input:[0.57 0.28 0.41 0.13]-- Real:[0 1 0] predict: [[0. 0. 0.]] predict_float:[[0.02 0.48 0.5 ]]\u001b[0m\n",
      "Input:[0.63 0.33 0.6  0.25]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.58 0.27 0.51 0.19]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.71 0.3  0.59 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.63 0.29 0.56 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.65 0.3  0.58 0.22]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.76 0.3  0.66 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.49 0.25 0.45 0.17]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.02 0.98]]\n",
      "Input:[0.73 0.29 0.63 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.67 0.25 0.58 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.72 0.36 0.61 0.25]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.65 0.32 0.51 0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.64 0.27 0.53 0.19]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.68 0.3  0.55 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.57 0.25 0.5  0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.58 0.28 0.51 0.24]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.64 0.32 0.53 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.65 0.3  0.55 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.77 0.38 0.67 0.22]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.77 0.26 0.69 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.6  0.22 0.5  0.15]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.69 0.32 0.57 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.56 0.28 0.49 0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.77 0.28 0.67 0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.63 0.27 0.49 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.67 0.33 0.57 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.72 0.32 0.6  0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.62 0.28 0.48 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.61 0.3  0.49 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.64 0.28 0.56 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.72 0.3  0.58 0.16]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.74 0.28 0.61 0.19]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.79 0.38 0.64 0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.64 0.28 0.56 0.22]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.63 0.28 0.51 0.15]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.61 0.26 0.56 0.14]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.77 0.3  0.61 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.63 0.34 0.56 0.24]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.64 0.31 0.55 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.6  0.3  0.48 0.18]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0.   0.01 0.99]]\n",
      "Input:[0.69 0.31 0.54 0.21]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.67 0.31 0.56 0.24]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.69 0.31 0.51 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.58 0.27 0.51 0.19]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.68 0.32 0.59 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.67 0.33 0.57 0.25]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.67 0.3  0.52 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.63 0.25 0.5  0.19]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.65 0.3  0.52 0.2 ]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "Input:[0.62 0.34 0.54 0.23]-- Real:[0 0 1] predict: [[0. 0. 1.]] predict_float:[[0. 0. 1.]]\n",
      "::::::::::::::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_predict(cnn,Xtrain,Ytrain)\n",
    "print(\"::::::::::::::::::::::::::::\")\n",
    "\n",
    "\n",
    "np.round(cnn.Predict(np.array([0.51,0.35,0.14,0.02])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
