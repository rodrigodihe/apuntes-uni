{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIONES DE ACTIVACIÓN PARA REDES NEURONALES MULTICAPA\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit,softmax\n",
    "\n",
    "class function(object):\n",
    "    def __init__(self,funcion,derivative=None,rand_init=(0,1)):\n",
    "        self.F=funcion\n",
    "        self.D=derivative\n",
    "        self.Rand_init=rand_init\n",
    "\n",
    "lineal=function(funcion=lambda x:x,\n",
    "                derivative=lambda x:1,\n",
    "                rand_init=(-1,1))\n",
    "\n",
    "sigm=function(funcion=lambda x: expit(x),\n",
    "              derivative=lambda x: expit(x)*(1-expit(x)),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "tanh=function(funcion=lambda x:(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)),\n",
    "              derivative=lambda x:1-((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))**2,\n",
    "              rand_init=(-1,1))\n",
    "\n",
    "tanh1=function(funcion=lambda x:np.tanh(x),\n",
    "               derivative=lambda x:1-np.tanh(x)**2,\n",
    "               rand_init=(-1,1))\n",
    "\n",
    "relu=function(funcion=lambda x: np.maximum(0, x),\n",
    "              derivative=lambda x: np.where(x<=0,0,1),\n",
    "              rand_init=(0,1))\n",
    "\n",
    "softmaxf=function(funcion=lambda x: softmax(x),\n",
    "                  derivative=lambda x:softmax(x)*(1-softmax(x)),\n",
    "                  rand_init=(0,1))\n",
    "\n",
    "\n",
    "# funciones de coste\n",
    "mse=function(funcion=lambda Yp, Yr: np.mean((Yp - Yr) ** 2) ,\n",
    "                 derivative=lambda Yp, Yr: (Yp - Yr))\n",
    "\n",
    "cross_entropy=function(funcion=lambda yscore,yreal:-np.sum(yreal*np.log(yscore))/yscore.shape[0],\n",
    "                       derivative=lambda yscore,yreal:yscore-yreal)\n",
    "\n",
    "Funciones={\"relu\":relu,\n",
    "           \"sigm\":sigm,\n",
    "           \"relu\":relu,\n",
    "           \"tanh\":tanh,\n",
    "           \"tanh1\":tanh1,\n",
    "           \"lineal\":lineal,\n",
    "           \"softmax\":softmaxf}\n",
    "\n",
    "Loss={\"mse\":mse,\n",
    "      \"cross_entropy\":cross_entropy}\n",
    "\n",
    "_x = np.linspace(-10, 10, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "Min_Max = preprocessing.MinMaxScaler()\n",
    "Ordinal =preprocessing.OrdinalEncoder()\n",
    "\n",
    "\n",
    "def one_hot_cols(df,cols_to_one):\n",
    "    one_hot=pd.get_dummies(df,cols_to_one,columns=cols_to_one)\n",
    "    return one_hot\n",
    "\n",
    "def fit_cols(df, cols_to_fit,fit_function ):\n",
    "    for col in cols_to_fit:\n",
    "        df[col] = pd.DataFrame(fit_function.fit_transform(pd.DataFrame(df[col])),columns=[col])\n",
    "    return df\n",
    "\n",
    "def split_Dataset(mypandas, cols_for_Y,size=0.2,state=1):\n",
    "    \n",
    "    X =  mypandas.drop(cols_for_Y, axis=1)\n",
    "    Y = mypandas[cols_for_Y]\n",
    "    X.head()\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size, random_state=state)\n",
    "    X_train, X_testVal, Y_train, Y_testVal = train_test_split(X_train, Y_train, test_size=0.1, random_state=state)\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy(), Y_test.to_numpy(), X_testVal.to_numpy(), Y_testVal.to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5)\n",
      "(288, 5)\n",
      "(288, 2)\n",
      "(80, 5)\n",
      "(80, 2)\n",
      "(32, 5)\n",
      "(32, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.12188842, 0.66666667, 0.43703704, 1.        , 0.        ],\n",
       "       [0.91978177, 0.71428571, 0.88148148, 1.        , 0.        ],\n",
       "       [0.11294846, 0.5       , 0.44444444, 1.        , 0.        ],\n",
       "       ...,\n",
       "       [0.94537049, 0.07142857, 0.42222222, 0.        , 1.        ],\n",
       "       [0.14737655, 0.35714286, 0.20740741, 0.        , 1.        ],\n",
       "       [0.65983094, 0.54761905, 0.27407407, 0.        , 1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hacer aquí la normalizacion de los datos\n",
    "#modificar el split para hacer el vaidation \n",
    "import csv\n",
    "data = pd.read_csv('datasets/Social_Network_Ads.csv', encoding='utf-8' )\n",
    "data\n",
    "print(data.shape)\n",
    "data=one_hot_cols(data,['Purchased','Gender'])\n",
    "dataset= fit_cols(data,data.columns,Min_Max)\n",
    "Xtrain,X_test , Ytrain, Y_test, X_trainVal, Y_trainVal= split_Dataset(data,['Purchased_0','Purchased_1'])\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_trainVal.shape)\n",
    "print(Y_trainVal.shape)\n",
    "display(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASE DE LA CAPA DE LA RED\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class neural_layer(object):\n",
    "    def __init__(self, n_conn, n_neur, activation=\"relu\"):\n",
    "        self.act = Funciones[activation]\n",
    "        self.activation=activation\n",
    "        self.random=self.act.Rand_init  \n",
    "        self.shape=(n_conn,n_neur)\n",
    "        self.Initialize()\n",
    "        \n",
    "    def show(self,Full=False):\n",
    "        print(f\"Pesos shape:{np.shape(self.W)} bias shape:{np.shape(self.b)} Activation:{self.activation}\")\n",
    "        print(f\"Activation:{self.activation}, Random:{self.random}\")\n",
    "        print(\"______________________\")\n",
    "        if Full:\n",
    "            print(f\"Pesos:\")\n",
    "            print(self.W)\n",
    "            print(\"#####\")\n",
    "            print(f\"Bias:\")\n",
    "            print(self.b)\n",
    "            \n",
    "    def Initialize(self):\n",
    "        #inicializa los pesos iniciales con aleatorios\n",
    "        self.b = np.random.uniform(*self.random,(1, self.shape[1]))      \n",
    "        self.W = np.random.uniform(*self.random,self.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "#CLASE RED NEURONAL MULTICAPA        \n",
    "class Neural_Net(object):\n",
    "    def __init__(self,Input,loss):\n",
    "        self.loss = Loss[loss]\n",
    "        self.Funcion_Loss=loss\n",
    "        self.Input=Input\n",
    "        self.NN=None;\n",
    "              \n",
    "    def Add_Layer(self,Num_neurons, function):\n",
    "        if self.NN is None:\n",
    "            self.NN=[]\n",
    "            self.NN.append(neural_layer(self.Input,Num_neurons,function))\n",
    "        else:\n",
    "            _,L_input=np.shape(self.NN[-1].W)\n",
    "            self.NN.append(neural_layer(L_input,Num_neurons,function))\n",
    "            \n",
    "    def Show_Model(self, Full=False):\n",
    "        print(f\"Input shape:{self.Input}, Loss: {self.Funcion_Loss}\")\n",
    "        for i,L in enumerate(self.NN):\n",
    "            print(F\"Layer_{i}:\")\n",
    "            L.show(Full)\n",
    "            \n",
    "            \n",
    "    # fucnción de predicción (fordware pass)    \n",
    "    def Predict(self,X):  \n",
    "      #sólo podemos pasar Numpy  \n",
    "      sx=np.shape(X)\n",
    "      X=X.reshape(1,sx[0])\n",
    "      if self.NN is None:\n",
    "          print(\"error in Predict Method ( not NEURAL network available)\")\n",
    "          return 0\n",
    "        \n",
    "      out = [(None, X)] #primer data necesario\n",
    "      # Forward pass\n",
    "      for l, layer in enumerate(self.NN):\n",
    "          z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "          a = self.NN[l].act.F(z)\n",
    "          out.append((z, a))\n",
    "      return out[-1][1]\n",
    "    \n",
    "    \n",
    "    # función retropropagación del error\n",
    "    def _backward_pass(self, X, Y,lr=0.01):\n",
    "      sx=np.shape(X)\n",
    "      sy=np.shape(Y)   \n",
    "      X=X.reshape(1,sx[0])\n",
    "      Y=Y.reshape(1,sy[0])\n",
    "\n",
    "      # Forward pass\n",
    "      out = [(None, X)] #primer data necesario\n",
    "      for l, layer in enumerate(self.NN):\n",
    "            z = out[-1][1] @ self.NN[l].W + self.NN[l].b\n",
    "            a = self.NN[l].act.F(z)\n",
    "            out.append((z, a))\n",
    "\n",
    "      # Backward pass \n",
    "      deltas = []\n",
    "      for l in reversed(range(0, len(self.NN))):\n",
    "        z = out[l+1][0]\n",
    "        a = out[l+1][1]\n",
    "        if l == len(self.NN) - 1:\n",
    "            deltas.insert(0, self.loss.D(a, Y) * self.NN[l].act.D(a)) # La última capa\n",
    "        else:\n",
    "            deltas.insert(0, deltas[0] @ _W.T * self.NN[l-1].act.D(a))\n",
    "        _W = self.NN[l].W #los pesos en la capa superior\n",
    " \n",
    "        # Gradient descent. actualizamos pesos \n",
    "        self.NN[l].b = self.NN[l].b - (deltas[0]* lr)\n",
    "        self.NN[l].W = self.NN[l].W - (lr * (out[l][1].T @ deltas[0]))\n",
    "      return out[-1][1]\n",
    "\n",
    "    # función de entrenamiento de la red\n",
    "    def Train(self,X,Y,valX=None,valY=None,lr=0.01,epoch=10,batch_size=1):\n",
    "        H_loss = []\n",
    "        H_acc=[]\n",
    "        H_ValAcc=[]\n",
    "        H_ValLoss=[]\n",
    "        # inicializamos las capas neuronales a valores ramdom del rango de la función\n",
    "        for Layer in self.NN:\n",
    "            Layer.Initialize()\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            account=0\n",
    "            epoch_Loss=0\n",
    "            epoch_Acc=0\n",
    "            loss_Val=0\n",
    "            acc_val=0\n",
    "            k=0\n",
    "            # Entrenemos a la red! con el dataset de validación\n",
    "            for j in range(len(X)):\n",
    "                pY = self._backward_pass(X[j,:], Y[j,:],lr)#fila, fila, learning rate\n",
    "                epoch_Loss+=self.loss.F(pY[0],Y[j,:])\n",
    "                if (Y[j,:]==np.round(pY)).all():#condicion de acertar\n",
    "                    epoch_Acc+=1\n",
    "                \n",
    "            for k in range(len(valX)):\n",
    "                pY=self.Predict(valX[k])\n",
    "                loss_Val+=self.loss.F(pY[0],valY[k,:])\n",
    "                if (valY[k,:]==np.round(pY)).all():#condicion de acertar\n",
    "                    acc_val+=1\n",
    "                    \n",
    "            H_ValAcc.append(acc_val/len(valY)*100)    \n",
    "            H_ValLoss.append(loss_Val/len(valY))#media del error  \n",
    "            H_acc.append(epoch_Acc/len(Y)*100)    \n",
    "            H_loss.append(epoch_Loss/len(Y))#media del error  \n",
    "            \n",
    "            \n",
    "            #imprimimos por pantalla resultados\n",
    "            print(\"Epoch={}, Accurary={} Loss={} Vacc={} Vloss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7),round(H_ValAcc[-1],3),round(H_ValLoss[-1],7)))\n",
    "            clear_output(wait=True)\n",
    "        print(\"Epoch={}, Accurary={} Loss={} Vacc={} Vloss={}\".format(i,round(H_acc[-1],3),round(H_loss[-1],7),round(H_ValAcc[-1],3),round(H_ValLoss[-1],7)))\n",
    "        return H_loss,H_acc,H_ValLoss,H_ValAcc\n",
    "\n",
    "    \n",
    "# VISUALIZACIÓN Y TEST\n",
    "def Show_Loss_Acc(H_loss,H_acc,V_loss,V_acc):\n",
    "    plt.plot(range(len(H_loss)), H_loss,\"tab:blue\")\n",
    "    plt.plot(range(len(V_loss)), V_loss,\"tab:green\")\n",
    "    plt.ylabel(\"loss function \")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "    plt.plot(range(len(H_acc)), H_acc, \"tab:red\")\n",
    "    plt.plot(range(len(V_acc)), V_acc, \"tab:green\")\n",
    "    plt.ylabel(\"ACCURACY\")\n",
    "    plt.xlabel(\"EPOCH NUMBER\")\n",
    "    plt.show()\n",
    "       \n",
    "def print_predict(neural_net,X,Y):\n",
    "    for i in range(len(X)):\n",
    "        sal_float=neural_net.Predict(X[i])\n",
    "        sal=np.round(sal_float)\n",
    "        \n",
    "        if (Y[i]==np.round(sal)).all():\n",
    "            print(\"Input:{}-- Real:{} predict: {} predict_float:{}\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "        else:\n",
    "            print(\"\\x1b[31m Input:{}-- Real:{} predict: {} predict_float:{}\\x1b[0m\".format(X[i],Y[i],sal,np.round(sal_float,2)))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINIMOS LOS MODELOs\n",
    "def Model1():\n",
    "    red=Neural_Net(Input=5,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(10,\"relu\")\n",
    "    red.Add_Layer(2,\"softmax\")\n",
    "    return red\n",
    "\n",
    "def Model2():\n",
    "    red=Neural_Net(Input=5,loss=\"mse\")\n",
    "    red.Add_Layer(15,\"tanh\")\n",
    "    red.Add_Layer(20,\"tanh1\")\n",
    "    red.Add_Layer(2,\"sigm\")\n",
    "    return red\n",
    "\n",
    "def Model3(): \n",
    "    red=Neural_Net(Input=5,loss=\"cross_entropy\")\n",
    "    red.Add_Layer(10,\"sigm\")\n",
    "    red.Add_Layer(20,\"tanh\")\n",
    "    red.Add_Layer(8,\"relu\")\n",
    "    red.Add_Layer(2,\"softmax\")\n",
    "    return red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:5, Loss: mse\n",
      "Layer_0:\n",
      "Pesos shape:(5, 15) bias shape:(1, 15) Activation:tanh\n",
      "Activation:tanh, Random:(-1, 1)\n",
      "______________________\n",
      "Layer_1:\n",
      "Pesos shape:(15, 20) bias shape:(1, 20) Activation:tanh1\n",
      "Activation:tanh1, Random:(-1, 1)\n",
      "______________________\n",
      "Layer_2:\n",
      "Pesos shape:(20, 2) bias shape:(1, 2) Activation:sigm\n",
      "Activation:sigm, Random:(0, 1)\n",
      "______________________\n",
      "Input shape:5, Loss: mse\n",
      "Layer_0:\n",
      "Pesos shape:(5, 15) bias shape:(1, 15) Activation:tanh\n",
      "Activation:tanh, Random:(-1, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[-0.85318523  0.23570234 -0.33642116  0.13401446 -0.87242458 -0.28078142\n",
      "  -0.73005761 -0.99338808  0.01043908  0.98401579 -0.42809832 -0.48373266\n",
      "   0.72524813  0.68019429 -0.66596748]\n",
      " [ 0.25741718 -0.74547267  0.98527341 -0.54724654 -0.0251593   0.94003873\n",
      "  -0.22788193  0.28228942 -0.57639133 -0.64860171 -0.27535057  0.52631172\n",
      "   0.9299623   0.5348852  -0.62305274]\n",
      " [-0.31109467  0.46172769  0.12886409  0.50949849  0.18808349 -0.99863589\n",
      "  -0.40472407  0.56639377  0.51627014 -0.87955209  0.0098031  -0.53980559\n",
      "  -0.37400527 -0.88022376 -0.51626266]\n",
      " [ 0.05545643  0.48257402  0.45581221  0.42128933  0.52186297  0.90810393\n",
      "  -0.0409766  -0.62945287 -0.09770828 -0.30580711  0.85467768  0.87399079\n",
      "   0.91691397  0.11631243 -0.62085158]\n",
      " [ 0.51513472 -0.05017186 -0.98100558 -0.83774042  0.17710507  0.59227372\n",
      "  -0.62239067  0.82636003 -0.74122077  0.62133536 -0.55722781  0.94932983\n",
      "   0.27161641  0.1527817  -0.05388556]]\n",
      "#####\n",
      "Bias:\n",
      "[[-0.79828496  0.76765494  0.18931023  0.84507108  0.25939175  0.40494411\n",
      "   0.81107727  0.42215229 -0.29483476 -0.84490961 -0.49173376  0.17929276\n",
      "   0.07366938 -0.1608287   0.42738877]]\n",
      "Layer_1:\n",
      "Pesos shape:(15, 20) bias shape:(1, 20) Activation:tanh1\n",
      "Activation:tanh1, Random:(-1, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[ 0.17148793  0.32160316  0.82569388 -0.97163608  0.61584083 -0.99555269\n",
      "   0.97834881  0.34121785  0.58361046 -0.3062325  -0.71705444 -0.07427861\n",
      "  -0.08317511 -0.10292906  0.04288154  0.20047774  0.17734828  0.0681688\n",
      "   0.28420522 -0.96361724]\n",
      " [ 0.30027302  0.85836192 -0.76724953  0.21391988 -0.508527    0.17271261\n",
      "   0.05533575 -0.68605562  0.30410411 -0.4763143   0.00518709 -0.53005186\n",
      "   0.00412884 -0.86020533 -0.49624955  0.47768322  0.86846568  0.53547866\n",
      "   0.58889816 -0.44613544]\n",
      " [ 0.12607173  0.26064918 -0.1397495   0.65497427 -0.18819693 -0.70262215\n",
      "  -0.75568118  0.53259086 -0.96666019 -0.21654559 -0.81581736 -0.7772946\n",
      "  -0.28422027  0.88226067  0.26797587  0.31772178 -0.42362842 -0.53536907\n",
      "   0.60993805 -0.18123179]\n",
      " [-0.60127087 -0.24041624 -0.743594   -0.5439697  -0.77224945 -0.25819588\n",
      "   0.67908878 -0.12649065 -0.03407616  0.75801922 -0.49441927 -0.25225169\n",
      "  -0.45397574  0.50745334 -0.44198683 -0.70529749 -0.66144139 -0.52869837\n",
      "   0.14402899  0.6878303 ]\n",
      " [ 0.93595319 -0.31060391 -0.96433233  0.02465418 -0.79533302 -0.77317356\n",
      "  -0.01288351  0.4842262   0.67026873  0.8235571   0.16049869  0.7653215\n",
      "   0.50102197  0.4910793  -0.46811761 -0.93398811 -0.32203514  0.35454227\n",
      "   0.91021455  0.14315875]\n",
      " [-0.74654883  0.65858602  0.68328205  0.44046007 -0.10496932 -0.84001498\n",
      "   0.03901247  0.8681062   0.29980326  0.14747124  0.84960276 -0.40789975\n",
      "  -0.14458038  0.46030843  0.50961916 -0.33619855  0.65124732  0.45293044\n",
      "   0.54936178 -0.03223863]\n",
      " [ 0.94385638 -0.36414596  0.30241684 -0.94864318  0.15170829  0.92054671\n",
      "   0.04853523 -0.17308078 -0.77997465  0.19720382 -0.59845754  0.01773938\n",
      "  -0.00892503  0.09666163 -0.50841793 -0.59039213  0.34246415  0.22798928\n",
      "  -0.05024259 -0.73827792]\n",
      " [-0.91846568  0.21192964 -0.28228442 -0.02640949 -0.74684106  0.64655526\n",
      "  -0.23369665  0.2531011   0.31060077 -0.4606948  -0.32992842  0.49415128\n",
      "   0.29916763  0.55654803 -0.08408641 -0.14846004  0.6541786   0.75668087\n",
      "  -0.85810858  0.68621012]\n",
      " [ 0.68897448 -0.06610852 -0.70739071 -0.40375617  0.91845357 -0.548546\n",
      "  -0.25245003  0.4542736   0.00584338  0.73829117  0.28848369  0.56927308\n",
      "  -0.31647611 -0.29965405 -0.80371783 -0.91481849  0.50994916 -0.41426452\n",
      "   0.92379262 -0.6809101 ]\n",
      " [-0.29583958  0.09648882 -0.48197002  0.1383536   0.84518162 -0.27415371\n",
      "  -0.06277535 -0.12942114  0.13985837  0.7446669   0.90260411  0.16831267\n",
      "   0.2606517  -0.25812799  0.04714911 -0.11050384  0.20646339 -0.18924522\n",
      "   0.58097167 -0.98223585]\n",
      " [ 0.79207835  0.33384491  0.69725142  0.90438216 -0.41883857  0.83337137\n",
      "  -0.84762574  0.69616237 -0.7342971   0.0861218   0.5531385   0.69052388\n",
      "   0.09549332  0.37275182  0.94930302 -0.29902183  0.20015758 -0.80524022\n",
      "   0.02429493  0.64392441]\n",
      " [ 0.29621095 -0.66914538 -0.53666822  0.20993723  0.17668885  0.96448182\n",
      "   0.31090764 -0.87758026 -0.3862893   0.08795348  0.14985537  0.02673717\n",
      "   0.70942246 -0.72351027 -0.85651641  0.06615031  0.07305078 -0.87122375\n",
      "  -0.358162    0.77071699]\n",
      " [ 0.2338157  -0.28403776  0.86824484 -0.68354654 -0.73007559  0.43304538\n",
      "  -0.52863969 -0.61550323 -0.07555316  0.67952291 -0.89583418  0.07331109\n",
      "   0.84925343 -0.12898493 -0.96299492  0.88483365  0.10534062  0.28523417\n",
      "   0.62708664 -0.21553239]\n",
      " [ 0.40762748 -0.24796934  0.18236427 -0.23003987  0.28717751 -0.76899793\n",
      "  -0.566698    0.26316876 -0.66402041 -0.50994103 -0.79060804 -0.84367528\n",
      "  -0.04965431  0.08008405  0.55041858  0.9308667  -0.41681572 -0.21950841\n",
      "  -0.25969406 -0.50669667]\n",
      " [ 0.3981737   0.20604751 -0.55252254  0.83267282  0.64458357  0.32650805\n",
      "   0.27279143 -0.35308866 -0.85194378  0.43635683 -0.77527061 -0.37541907\n",
      "  -0.0164691   0.58045739  0.93660429  0.17463    -0.56675036 -0.68249199\n",
      "  -0.5862858   0.67931775]]\n",
      "#####\n",
      "Bias:\n",
      "[[ 0.59503202 -0.3898196  -0.97626151  0.04390314  0.33587326  0.36709005\n",
      "   0.74360082 -0.27005541  0.17178939  0.82092225  0.15553187  0.83577649\n",
      "   0.41919009 -0.5873292   0.13200052 -0.26121475 -0.97489273  0.04083565\n",
      "  -0.69772472  0.8805595 ]]\n",
      "Layer_2:\n",
      "Pesos shape:(20, 2) bias shape:(1, 2) Activation:sigm\n",
      "Activation:sigm, Random:(0, 1)\n",
      "______________________\n",
      "Pesos:\n",
      "[[0.41515388 0.6728343 ]\n",
      " [0.80949619 0.97309085]\n",
      " [0.50672357 0.77047678]\n",
      " [0.05901253 0.78087731]\n",
      " [0.60359936 0.21227281]\n",
      " [0.69616371 0.8115153 ]\n",
      " [0.84826572 0.0173564 ]\n",
      " [0.11662344 0.02663715]\n",
      " [0.55560155 0.35328698]\n",
      " [0.20906411 0.26007721]\n",
      " [0.60893807 0.94289507]\n",
      " [0.22133473 0.77544365]\n",
      " [0.88780221 0.17901829]\n",
      " [0.8780608  0.06168376]\n",
      " [0.10425934 0.27765571]\n",
      " [0.08766641 0.65973228]\n",
      " [0.61472203 0.32095264]\n",
      " [0.37919858 0.66508861]\n",
      " [0.08903491 0.53482464]\n",
      " [0.96546128 0.6419487 ]]\n",
      "#####\n",
      "Bias:\n",
      "[[0.64686835 0.02056054]]\n"
     ]
    }
   ],
   "source": [
    "cnn=Model2()\n",
    "cnn.Show_Model()\n",
    "cnn.Show_Model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=99, Accurary=86.458 Loss=0.0979517 Vacc=84.375 Vloss=0.103542\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "\n",
    "\n",
    "loss,accuracy,Vloss,Vaccuracy=cnn.Train(Xtrain,Ytrain,X_testVal,Y_testVal,0.01,100)\n",
    "Show_Loss_Acc(loss,accuracy,Vloss,Vaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.12188842 0.66666667 0.43703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.6 0.3]]\n",
      "Input:[0.91978177 0.71428571 0.88148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "\u001b[31m Input:[0.11294846 0.5        0.44444444 1.         0.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.84 0.11]]\u001b[0m\n",
      "Input:[0.71708771 0.26190476 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.38849795 0.80952381 0.04444444 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.98]]\n",
      "Input:[0.19110269 0.73809524 0.52592593 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.99725203 0.92857143 0.33333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.49381405 0.28571429 0.25185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97923532 0.61904762 0.17777778 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.04]]\n",
      "Input:[0.5266006  0.35714286 0.4        1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.05038484 0.5        0.45925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.76 0.24]]\n",
      "Input:[0.5607873  0.78571429 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.1152096  0.30952381 0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.34317051 0.38095238 0.71851852 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.01641541 0.19047619 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.37760263 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.02]]\n",
      "Input:[0.96897569 0.97619048 0.1037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.05 0.99]]\n",
      "\u001b[31m Input:[0.14988312 0.78571429 0.88148148 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.43 0.6 ]]\u001b[0m\n",
      "Input:[0.11482335 0.04761905 0.05925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.90654484 0.28571429 0.         0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.07010746 0.14285714 0.05925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.94407496 0.57142857 0.68888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.05 0.95]]\n",
      "Input:[0.10591558 0.23809524 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.73504005 0.95238095 0.05925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 1.  ]]\n",
      "Input:[0.33312814 0.64285714 0.05185185 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.98]]\n",
      "Input:[0.47520992 0.0952381  0.48888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.18417442 0.4047619  0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.79 0.17]]\n",
      "Input:[0.70616423 0.64285714 0.12592593 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.94]]\n",
      "Input:[0.7800577  0.45238095 0.45925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.94 0.09]]\n",
      "Input:[0.87041887 0.28571429 0.54814815 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.13]]\n",
      "Input:[0.76823699 0.5        0.32592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.71834703 0.19047619 0.12592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.04553264 0.11904762 0.35555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.79628803 0.73809524 0.54074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.97]]\n",
      "Input:[0.53348059 0.26190476 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.80435491 0.69047619 0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.41 0.51]]\u001b[0m\n",
      "Input:[0.96879866 0.23809524 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.03064209 0.4047619  0.28148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.44865559 0.71428571 0.43703704 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.28 0.75]]\n",
      "Input:[0.48603282 0.28571429 0.68148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.98]]\n",
      "Input:[0.93894515 0.23809524 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.35312034 0.57142857 0.65925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.20594898 0.66666667 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.76 0.32]]\n",
      "Input:[0.19367765 0.4047619  0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.04]]\n",
      "Input:[0.40657501 0.38095238 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.23619678 0.88095238 0.17777778 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "\u001b[31m Input:[0.28527401 0.45238095 0.47407407 0.         1.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.69 0.37]]\u001b[0m\n",
      "Input:[0.45825538 0.42857143 0.95555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.50362306 0.66666667 0.19259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.34 0.67]]\n",
      "Input:[0.75001106 0.4047619  0.60740741 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.42 0.62]]\n",
      "Input:[0.45448949 0.19047619 0.         1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.57330807 0.57142857 0.28888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.06]]\n",
      "Input:[0.21233811 0.71428571 0.11111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.42124025 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.12]]\n",
      "\u001b[31m Input:[0.59233465 0.45238095 0.46666667 1.         0.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.95 0.1 ]]\u001b[0m\n",
      "Input:[0.13633639 0.16666667 0.13333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.85767682 0.26190476 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.52705122 0.26190476 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.56707584 0.73809524 0.0962963  0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.10536438 0.33333333 0.02222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.67441168 0.47619048 0.41481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.05]]\n",
      "Input:[0.83701674 0.30952381 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.29078605 0.07142857 0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.43022044 0.16666667 0.53333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.33171593 0.14285714 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.63453995 0.04761905 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.83481193 0.0952381  0.08888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.39516872 0.95238095 0.95555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.71551859 0.26190476 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.64624397 0.4047619  0.37037037 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.17292504 0.97619048 0.54074074 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.4  0.69]]\n",
      "Input:[0.08177125 0.07142857 0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.21884794 0.71428571 0.1037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.19204014 0.11904762 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.74090615 0.21428571 0.6        1.         0.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.95 0.04]]\u001b[0m\n",
      "Input:[0.51602715 0.14285714 0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.6690807  0.97619048 0.94814815 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.42231852 0.4047619  0.05925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.55693289 0.66666667 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 1.  ]]\n",
      "Input:[0.31946473 0.66666667 0.12592593 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.06 0.93]]\n",
      "Input:[0.51381027 0.14285714 0.12592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.78824528 0.19047619 0.11111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.90692304 0.19047619 0.51111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.06020994 0.38095238 0.20740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.38709781 0.02380952 0.40740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.41815029 0.16666667 0.55555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.02]]\n",
      "\u001b[31m Input:[0.73185353 0.64285714 0.22222222 1.         0.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.64 0.32]]\u001b[0m\n",
      "Input:[0.42700978 0.88095238 0.81481481 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.96]]\n",
      "Input:[0.59052815 0.54761905 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.09]]\n",
      "Input:[0.09893501 0.45238095 0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.89 0.1 ]]\n",
      "Input:[0.69391705 0.57142857 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.76 0.31]]\n",
      "Input:[0.2029596  0.4047619  0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.88860859 0.57142857 0.28888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.92 0.07]]\n",
      "Input:[0.9664651  0.92857143 0.13333333 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.07 0.98]]\n",
      "Input:[0.52830249 0.47619048 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.92 0.19]]\n",
      "Input:[0.09157222 0.5        0.67407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.4250343  0.69047619 0.14074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.99]]\n",
      "Input:[0.94228053 0.52380952 0.94074074 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "\u001b[31m Input:[0.63357031 0.76190476 0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.19 0.85]]\u001b[0m\n",
      "Input:[0.22588887 0.38095238 0.07407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.1041292  0.69047619 0.05925926 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.99]]\n",
      "Input:[0.77400653 0.35714286 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.39721662 0.5        0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.07]]\n",
      "Input:[0.1355478  0.71428571 0.55555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.90980378 0.73809524 0.0962963  0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.21081727 0.42857143 0.25925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "\u001b[31m Input:[0.43895923 0.64285714 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.25 0.72]]\u001b[0m\n",
      "Input:[0.93956475 0.57142857 0.28148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.6401405  0.42857143 0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.03623057 0.4047619  0.42962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.06]]\n",
      "Input:[0.71774755 0.19047619 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.34484826 0.47619048 0.25925926 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.02]]\n",
      "Input:[0.55767722 0.5952381  0.71851852 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.96]]\n",
      "Input:[0.99504319 0.21428571 0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.08170286 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.75 0.14]]\n",
      "\u001b[31m Input:[0.28058677 0.28571429 0.74814815 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.26 0.5 ]]\u001b[0m\n",
      "Input:[0.03834285 0.54761905 0.22222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.05]]\n",
      "Input:[0.1758782  0.23809524 0.32592593 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.58149565 0.4047619  0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.54 0.44]]\n",
      "Input:[0.98542328 0.42857143 0.44444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.21309451 0.47619048 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.01]]\n",
      "Input:[0.01758219 0.35714286 0.19259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.11220413 0.16666667 0.05185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97630629 0.54761905 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.91948404 0.42857143 0.82222222 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.94]]\n",
      "Input:[0.80943242 0.04761905 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.72199624 0.04761905 0.52592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.47816711 0.4047619  0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.61982643 0.45238095 0.9037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.86952166 0.21428571 0.11851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.42068502 0.66666667 0.54074074 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.6251212  0.78571429 0.97037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.67268565 0.69047619 0.07407407 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 1.  ]]\n",
      "Input:[0.78560997 0.71428571 0.91111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.58924469 0.4047619  0.08888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.  ]]\n",
      "Input:[0.46501869 0.4047619  0.25925926 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.40013358 0.         0.39259259 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.84553425 0.4047619  0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.04]]\n",
      "Input:[1.         0.64285714 0.85925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.35580393 0.19047619 0.01481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.         0.4047619  0.31111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.02]]\n",
      "Input:[0.93407283 0.16666667 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.75311712 0.45238095 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.05]]\n",
      "Input:[0.25367436 0.69047619 0.26666667 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.09 0.9 ]]\n",
      "Input:[0.90318934 0.66666667 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.71 0.22]]\n",
      "Input:[0.81323452 0.19047619 0.48888889 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.13214402 0.45238095 0.13333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.1 ]]\n",
      "Input:[0.87903294 0.57142857 0.44444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.04]]\n",
      "Input:[0.8961806  0.54761905 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.05]]\n",
      "Input:[0.63039988 0.45238095 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.51404362 0.5        0.2        0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.02]]\n",
      "Input:[0.40964083 0.04761905 0.4962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.60387774 0.54761905 0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.05]]\n",
      "Input:[0.22858051 0.42857143 0.33333333 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.  ]]\n",
      "Input:[0.72440223 0.         0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.1209912  0.42857143 0.81481481 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.93330436 0.28571429 0.34814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.07093628 0.45238095 0.97037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.96]]\n",
      "Input:[0.51897629 0.54761905 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.06]]\n",
      "Input:[0.17279629 0.54761905 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.84 0.16]]\n",
      "Input:[0.27843426 1.         0.2        0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.05837528 0.30952381 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "\u001b[31m Input:[0.02911723 0.52380952 0.41481481 0.         1.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.91 0.17]]\u001b[0m\n",
      "Input:[0.45306522 0.4047619  0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.04]]\n",
      "Input:[0.98273164 0.4047619  0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "Input:[0.35491074 0.21428571 0.31851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.97052067 0.02380952 0.51851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.24878192 0.4047619  0.17777778 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.06186757 0.52380952 0.46666667 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.62 0.34]]\n",
      "Input:[0.73887836 0.45238095 0.31111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.63637461 0.21428571 0.55555556 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.02]]\n",
      "Input:[0.40992649 0.19047619 0.20740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.50746941 0.92857143 0.08148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.99]]\n",
      "Input:[0.80306743 0.21428571 0.54074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "Input:[0.47545132 0.4047619  0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.04472796 0.54761905 0.53333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.07 0.84]]\n",
      "Input:[0.23362181 0.85714286 0.08148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.25058842 0.         0.4962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.85654625 0.26190476 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.32286449 0.83333333 0.42222222 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.05 0.94]]\n",
      "Input:[0.14357043 0.26190476 0.44444444 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.7955437  0.21428571 0.01481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.47101353 0.57142857 0.36296296 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.06]]\n",
      "Input:[0.43652106 0.30952381 0.31851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.35221105 0.88095238 0.85185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.07 0.92]]\n",
      "Input:[0.59533609 0.71428571 0.19259259 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.97]]\n",
      "Input:[0.84508765 0.23809524 0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.84048892 0.92857143 0.79259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.83760013 0.80952381 0.91111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.26893103 0.69047619 0.11111111 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.02 0.97]]\n",
      "Input:[0.12935582 0.33333333 0.88888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.07 0.89]]\n",
      "Input:[0.98215227 0.47619048 0.2962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.12625781 0.52380952 0.33333333 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.86 0.15]]\n",
      "Input:[0.53333172 0.30952381 0.45185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.94371688 0.35714286 0.11851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.91380705 0.30952381 0.54814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "\u001b[31m Input:[0.89488105 0.33333333 0.75555556 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.42 0.54]]\u001b[0m\n",
      "Input:[0.25938756 0.69047619 0.03703704 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.91411685 0.45238095 0.48148148 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "Input:[0.43308509 0.54761905 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.03]]\n",
      "Input:[0.75306481 0.73809524 0.93333333 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.57448692 0.04761905 0.25185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.2789573  0.73809524 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[0. 0.]] predict_float:[[0.38 0.49]]\u001b[0m\n",
      "\u001b[31m Input:[0.73932898 0.45238095 0.40740741 0.         1.        ]-- Real:[0. 1.] predict: [[1. 0.]] predict_float:[[0.98 0.04]]\u001b[0m\n",
      "Input:[0.08606421 0.83333333 0.65925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.31 0.6 ]]\n",
      "Input:[0.43959493 0.35714286 0.99259259 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.98]]\n",
      "Input:[0.17482005 0.30952381 0.         1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.85633703 1.         0.68888889 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.73775584 0.80952381 0.55555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.09 0.92]]\n",
      "Input:[0.97023098 0.71428571 0.13333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.98]]\n",
      "Input:[0.49688389 0.35714286 0.0962963  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.5717309  0.97619048 0.85185185 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.72563338 0.66666667 0.75555556 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.51337976 0.33333333 0.75555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.05 0.93]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:[0.06617662 0.52380952 0.34074074 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.04]]\n",
      "Input:[0.18001022 1.         0.22962963 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.4181342  0.71428571 0.13333333 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.88636757 0.26190476 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "\u001b[31m Input:[0.33385637 0.54761905 0.47407407 0.         1.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.45 0.58]]\u001b[0m\n",
      "Input:[0.50340177 0.52380952 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.11]]\n",
      "Input:[0.5259971  0.66666667 0.05185185 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.59 0.48]]\n",
      "Input:[0.42276511 0.19047619 0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.62062306 0.19047619 0.52592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.79464246 0.         0.21481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.84037627 0.11904762 0.24444444 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.29204939 0.11904762 0.03703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.51555641 0.33333333 1.         1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.30592202 0.52380952 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.02]]\n",
      "Input:[0.6807123  0.80952381 1.         0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.26241717 0.23809524 0.51851852 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.02]]\n",
      "Input:[0.91557734 0.4047619  0.97777778 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.98]]\n",
      "Input:[0.65886533 0.14285714 0.51111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.99145836 0.52380952 0.68148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.05871324 0.45238095 0.43703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.93 0.1 ]]\n",
      "Input:[0.56946573 0.4047619  0.05185185 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.94 0.  ]]\n",
      "Input:[0.72313486 0.14285714 0.54814815 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.84353865 0.21428571 0.28888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.65325673 0.21428571 0.9037037  1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.96]]\n",
      "Input:[0.17286067 0.4047619  0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1.   0.01]]\n",
      "\u001b[31m Input:[0.15413986 0.97619048 0.5037037  1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.4  0.71]]\u001b[0m\n",
      "Input:[0.68212048 0.47619048 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "Input:[0.15545551 0.52380952 0.23703704 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.91 0.  ]]\n",
      "Input:[0.89772156 0.26190476 0.98518519 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.80866798 0.4047619  0.42222222 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.22489107 0.5        0.6        0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.49512567 0.23809524 0.54814815 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.92 0.08]]\n",
      "Input:[0.37623065 0.45238095 0.41481481 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.97 0.04]]\n",
      "Input:[0.68280044 0.95238095 0.17037037 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.04 0.99]]\n",
      "Input:[0.59888874 0.23809524 0.51111111 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.01]]\n",
      "\u001b[31m Input:[0.86841121 0.35714286 0.72592593 1.         0.        ]-- Real:[1. 0.] predict: [[0. 1.]] predict_float:[[0.38 0.64]]\u001b[0m\n",
      "Input:[0.76542062 0.16666667 0.48148148 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.38814389 0.4047619  0.56296296 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.11 0.9 ]]\n",
      "Input:[0.70151722 0.45238095 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.04]]\n",
      "Input:[0.55642595 0.33333333 0.02222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.  ]]\n",
      "Input:[0.1100476  0.73809524 0.15555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.14025919 0.42857143 0.28888889 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.03]]\n",
      "Input:[0.54425521 0.54761905 0.32592593 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.03]]\n",
      "Input:[0.76497001 0.23809524 0.16296296 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.05267414 0.26190476 0.23703704 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.04220127 0.0952381  0.35555556 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.54254527 0.57142857 0.99259259 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.6910524  0.28571429 0.47407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.67938861 0.5        0.88148148 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.   0.99]]\n",
      "Input:[0.13032545 0.64285714 0.05185185 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.03 0.97]]\n",
      "Input:[0.42920655 0.30952381 0.14074074 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.36401164 0.4047619  0.31851852 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.  ]]\n",
      "Input:[0.6279255  0.28571429 0.01481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.71451275 0.33333333 0.62962963 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.13 0.88]]\n",
      "Input:[0.81637276 0.9047619  0.65925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.01 0.99]]\n",
      "Input:[0.63507908 0.71428571 0.6        1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.08 0.82]]\n",
      "Input:[0.99831018 0.30952381 0.37777778 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.08243109 0.52380952 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.95 0.05]]\n",
      "Input:[0.83186681 0.45238095 0.2962963  0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.01]]\n",
      "Input:[0.85166588 0.85714286 0.40740741 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.05 0.98]]\n",
      "Input:[0.77649298 0.76190476 0.15555556 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.23 0.82]]\n",
      "Input:[0.85720206 0.47619048 0.41481481 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.04]]\n",
      "Input:[0.81024112 0.69047619 0.25925926 1.         0.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0.41 0.52]]\n",
      "Input:[0.49305765 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.99 0.02]]\n",
      "Input:[0.81113029 0.57142857 0.47407407 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.87 0.11]]\n",
      "Input:[0.23263608 0.02380952 0.02962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.02721015 0.07142857 0.00740741 1.         0.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.45492804 0.14285714 0.02962963 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.91654295 0.52380952 0.31111111 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.96 0.03]]\n",
      "Input:[0.08074127 0.5        0.88148148 0.         1.        ]-- Real:[0. 1.] predict: [[0. 1.]] predict_float:[[0. 1.]]\n",
      "Input:[0.17747951 0.57142857 0.37037037 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.89 0.05]]\n",
      "Input:[0.94537049 0.07142857 0.42222222 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.14737655 0.35714286 0.20740741 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[1. 0.]]\n",
      "Input:[0.65983094 0.54761905 0.27407407 0.         1.        ]-- Real:[1. 0.] predict: [[1. 0.]] predict_float:[[0.98 0.06]]\n",
      "::::::::::::::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_predict(cnn,Xtrain,Ytrain)\n",
    "print(\"::::::::::::::::::::::::::::\")\n",
    "\n",
    "\n",
    "np.round(cnn.Predict(np.array([0.51,0.35,0.14,0.02,0.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
