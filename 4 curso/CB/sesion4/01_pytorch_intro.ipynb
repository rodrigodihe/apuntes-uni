{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Introducci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:21:50.890591Z",
     "start_time": "2020-08-15T09:21:49.757500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬ø Qu√© es Pytorch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` es un framework de `redes neuronales`, un conjunto de librer√≠as y herramientas que nos hacen la vida m√°s f√°cil a la hora de dise√±ar, entrenar y poner en producci√≥n nuestros modelos de `Deep Learning`. Una forma sencilla de entender qu√© es `Pytorch` es la siguiente:\n",
    "\n",
    "$$ Pytorch = Numpy + Autograd + GPU $$\n",
    "\n",
    "Vamos a ver qu√© significa cada uno de estos t√©rminos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Quiz√°s la caracter√≠stica m√°s relevante de `Pytorch` es su facilidad de uso. Esto es debido a que sigue una interfaz muy similar a la de `NumPy`, y como nosotros ya sabemos trabajar con esta librer√≠a no deber√≠amos tener muchos problemas para aprender a trabajar con `Pytorch` üòÅ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "De la misma manera que en `NumPy` el objeto principal es el `ndarray`, en `Pytorch` el objeto principal es el `tensor`. Podemos definir un tensor de manera similar a como definimos un array, incluso podemos inicializar tensores a partir de arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:10.873767Z",
     "start_time": "2020-08-15T09:29:10.860741Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matriz de ceros, 5 filas y 3 columnas\n",
    "\n",
    "x = torch.zeros(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:37.744086Z",
     "start_time": "2020-08-15T09:30:37.715079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4085,  0.1492],\n",
       "         [-0.3065, -0.3972],\n",
       "         [-0.9104,  0.4946]],\n",
       "\n",
       "        [[-1.0368, -1.8317],\n",
       "         [-0.2983,  0.1285],\n",
       "         [-0.2682, -1.6739]],\n",
       "\n",
       "        [[ 0.1817, -0.4391],\n",
       "         [-1.5879,  0.4627],\n",
       "         [ 0.4037,  0.2123]],\n",
       "\n",
       "        [[ 0.8303,  0.2736],\n",
       "         [-1.5003,  0.7714],\n",
       "         [-0.8558,  0.1547]],\n",
       "\n",
       "        [[ 1.2789,  1.3012],\n",
       "         [ 0.3494,  2.0338],\n",
       "         [-1.3126, -0.1815]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor con valores aleatorios\n",
    "\n",
    "x = torch.randn(5, 3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:41.310781Z",
     "start_time": "2020-08-15T09:29:41.290558Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor a partir de lista \n",
    "\n",
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:05.108750Z",
     "start_time": "2020-08-15T09:30:05.090922Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tensor a partir de array\n",
    "\n",
    "a = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "x = torch.from_numpy(a)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y como puedes esperar, pr√°cticamente todos los conceptos que ya conocemos para trabajar con `NumPy` pueden aplicarse en `Pytorch`. Esto incluye operaciones aritm√©ticas, indexado y troceado, iteraci√≥n, vectorizaci√≥n y broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:32.797001Z",
     "start_time": "2020-08-15T09:33:32.788000Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6251,  0.2682, -0.0485],\n",
       "         [ 1.7609,  0.0925,  0.1266],\n",
       "         [ 0.3453, -1.6688,  0.4845]]),\n",
       " tensor([[-1.0122, -0.7118, -0.2288],\n",
       "         [ 2.0132, -1.6643,  0.8877],\n",
       "         [ 0.3299,  0.9017,  0.1258]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operaciones\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:36.035260Z",
     "start_time": "2020-08-15T09:33:36.017231Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3871, -0.4436, -0.2773],\n",
       "        [ 3.7740, -1.5718,  1.0144],\n",
       "        [ 0.6752, -0.7671,  0.6103]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:39.567010Z",
     "start_time": "2020-08-15T09:33:39.553201Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6373,  0.9800,  0.1804],\n",
       "        [-0.2523,  1.7568, -0.7611],\n",
       "        [ 0.0154, -2.5705,  0.3587]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:28.502097Z",
     "start_time": "2020-08-15T09:34:28.490089Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6251,  0.2682, -0.0485])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexado\n",
    "\n",
    "# primera fila\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:36.631008Z",
     "start_time": "2020-08-15T09:34:36.625987Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6251)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera fila, primera columna\n",
    "\n",
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:44.819616Z",
     "start_time": "2020-08-15T09:34:44.811625Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6251,  0.2682, -0.0485])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera columna\n",
    "\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:35:30.156589Z",
     "start_time": "2020-08-15T09:35:30.133710Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2682, -0.0485],\n",
       "        [ 0.0925,  0.1266]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troceado\n",
    "\n",
    "x[:-1, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Una funcionalidad importante del objeto `tensor` que utilizaremos muy a menudo es cambiar su forma. Esto lo conseguimos con la funci√≥n `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:12.220185Z",
     "start_time": "2020-08-15T09:37:12.209120Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:52.119999Z",
     "start_time": "2020-08-15T09:37:52.100991Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a√±adimos una dimensi√≥n extra\n",
    "\n",
    "x.view(1, 3, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:38:04.360901Z",
     "start_time": "2020-08-15T09:38:04.349867Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estiramos en una sola dimensi√≥n\n",
    "\n",
    "x.view(9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:15.139898Z",
     "start_time": "2020-08-15T09:39:15.130902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usamos -1 para asignar todos los valores restantes a una dimensi√≥n\n",
    "\n",
    "x.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Podemos transformar un `tensor` en un `array` con la funci√≥n `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:46.534626Z",
     "start_time": "2020-08-15T09:39:46.515264Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6251061 ,  0.26821104, -0.04845389],\n",
       "       [ 1.7608535 ,  0.09249022,  0.12663704],\n",
       "       [ 0.34528732, -1.6688324 ,  0.4844624 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, un `tensor` de `Pytorch` es muy similar a un `array` de `NumPy`. Aqu√≠ hemos visto alguna de la funcionalidad m√°s √∫til, puedes aprender m√°s [aqu√≠](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ya hemos visto que `Pytorch` es muy similar a `NumPy`, sin embargo su funcionalidad va m√°s all√° de una estructura de datos eficiente con la que podemos llevar a cabo operaciones (para eso ya nos basta con `NumPy`). La funcionalidad m√°s importante que `Pytorch` a√±ade es la conocidad como `autograd`, la cual nos proporciona la posibilidad de calcular derivadas de manera autom√°tica con respecto a cualquier `tensor`. Esto le da a `Pytorch` un gran potencial para dise√±ar `redes neuronales` complejas y entrenarlas utilizando algoritmos de gradientes sin tener que calcular todas estas derivadas manualmente (como hemos hecho en los posts anteriores). Para poder llevar a cabo estas operaciones, `Pytorch` va construyendo de manera din√°mica un `grafo computacional`. Cada vez que aplicamos una operaci√≥n sobre uno o varios tensores, √©stos se a√±aden al `grafo computacional` junto a la operaci√≥n en concreto. De esta manera, si queremos calcular la derivada de cualquier valor con respecto a cualquier tensor, simplemente tenemos que aplicar el algoritmo de `backpropagation` (que no es m√°s que la regla de la cadena de la derivada) en el `grafo`. Vamos a ilustrarlo con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:47.830227Z",
     "start_time": "2020-08-15T09:52:47.810220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(2., requires_grad=True)\n",
    "p = x + y\n",
    "\n",
    "z = torch.tensor(3., requires_grad=True)\n",
    "g = p * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En la celda anterior hemos definido tres `tensores`: $x$, $y$ y $z$. En primer lugar, para poder calcular derivadas con respecto a estos tensores necesitamos ponder su propiedad `requiers_grad` a `True`. Ahora, calculamos el tensor intermedio $p$ como $p = x+ y$ y luego usamos este valor para calcular el resultado final $g$ como $g = p*z$. Cada vez que aplicamos una operaci√≥n sobre un tensor que tiene su propiedad `requires_grad` a `True`, `Pytorch` ir√° construyendo el `grafo computacional`. Para este ejemplo, el grafo tendr√≠a la siguiente forma\n",
    "\n",
    "![](https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg)\n",
    "\n",
    "Si ahora queremos calcular las derivadas de $g$ con respecto a $x$, $y$ y $z$, es tan f√°cil como llamar a la funci√≥n `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:48.341446Z",
     "start_time": "2020-08-15T09:52:48.335900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En este punto, `Pytorch` ha aplicado el algoritmo de `backpropagation` encima del grafo computacional, calculando todas las derivadas.\n",
    "\n",
    "$$ \\frac{dg}{dz} = p $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:55:50.796060Z",
     "start_time": "2020-08-15T09:55:50.789058Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\frac{dp}{dx} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:32.429083Z",
     "start_time": "2020-08-15T09:56:32.423080Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\frac{dp}{dy} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:47.233861Z",
     "start_time": "2020-08-15T09:56:47.214278Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, el `grafo computacional` es una herramienta extraordinaria para dise√±ar `redes neuronales` de complejidad arbitraria. Con una simple funci√≥n, gracias al algoritmo de `backpropagation`, podemos calcular todas las derivadas de manera sencilla (cada nodo que representa una operaci√≥n solo necesita calcular su propia derivada de manera local) y optimizar el modelo con nuestro algoritmo de gradiente preferido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> üí° Sabiendo que el `perceptr√≥n` lleva a cabo la operaci√≥n $\\hat{y} = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$, ¬øte ves capaz de dibujar su grafo computacional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A√±adiendo `autograd` encima de `NumPy`, `Pytorch` nos ofrece todo lo que necesitamos para dise√±ar y entrenar `redes neuronales`. Puedes aprender m√°s sobre `autograd` [aqu√≠](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py). Sin embargo, si queremos entrenar redes muy grandes o utilizar datasets muy grandes (o ambas), el proceso de entrenamiento ser√° muy lento. Es aqu√≠ donde entra en juego el √∫ltimo elemento que hace de `Pytorch` lo que es. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:19:44.098334Z",
     "start_time": "2020-08-15T10:19:44.084325Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobar que podemos usar GPU\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:06.036817Z",
     "start_time": "2020-08-15T10:21:04.774034Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 283 ms, total: 488 ms\n",
      "Wall time: 48.3 ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000,10000)\n",
    "y = torch.randn(10000,10000)\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:08.211624Z",
     "start_time": "2020-08-15T10:21:06.778483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(10000,10000).cuda()\n",
    "y = torch.randn(10000,10000).cuda()\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, llevar a cabo operaciones con grandes tensores en una GPU en vez de la CPU puede resultar en una considerable reducci√≥n del tiempo de c√°lculo. Todas las siguientes maneras son v√°lidas para copiar un tensor en una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T11:05:29.078172Z",
     "start_time": "2020-08-15T11:05:29.064175Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-fbab5100b4f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.randn((10000,10000), device=\"cuda\")\n",
    "x = x.cuda()\n",
    "x = x.to(\"cuda\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y para volver a copiar un `tensor` de vuelta en la CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:23:37.518832Z",
     "start_time": "2020-08-15T10:23:37.359224Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = x.cpu()\n",
    "x = x.to(\"cpu\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, simplemente definiendo los `tensores` para los pesos y los datos y copi√°ndolos a la GPU podemos definir el `grafo computacional` de manera din√°mica aplicando operaciones sobre los tensores (multiplicamos por los pesos y sumamos el *bias*). Una vez tenemos la salida del `MLP` calculamos la funci√≥n de p√©rdida y llamando a la funci√≥n `backward` `Pytorch` se encarga de calcular todas las derivadas de manera autom√°tica. Una vez tenemos los gradientes con respecto a los pesos, podemos actualizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto una introducci√≥n a `Pytorch`, un framework de `redes neuronales` muy utilizado a d√≠a de hoy. Hemos visto que `Pytorch` es muy similar a `NumPy` y comparten gran parte de su sintaxis, lo cual es una ventaja si ya sabemos trabajar con `NumPy`. Adem√°s, a√±ade `autograd`, la capacidad de construir de manera din√°mica un `grafo computacional` de manera que en cualquier momento podemos calcular derivadas con respecto a cualquier tensor de manera autom√°tica. Por √∫ltimo, hemos visto como podemos ejecutar todas estas operaciones en una GPU para acelerar el proceso de entrenamiento de nuestros modelos de `Deep Learning`. Este es el n√∫cleo de `Pytorch`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
