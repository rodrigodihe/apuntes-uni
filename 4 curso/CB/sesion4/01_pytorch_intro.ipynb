{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Introducci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:21:50.890591Z",
     "start_time": "2020-08-15T09:21:49.757500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Qu칠 es Pytorch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` es un framework de `redes neuronales`, un conjunto de librer칤as y herramientas que nos hacen la vida m치s f치cil a la hora de dise침ar, entrenar y poner en producci칩n nuestros modelos de `Deep Learning`. Una forma sencilla de entender qu칠 es `Pytorch` es la siguiente:\n",
    "\n",
    "$$ Pytorch = Numpy + Autograd + GPU $$\n",
    "\n",
    "Vamos a ver qu칠 significa cada uno de estos t칠rminos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Quiz치s la caracter칤stica m치s relevante de `Pytorch` es su facilidad de uso. Esto es debido a que sigue una interfaz muy similar a la de `NumPy`, y como nosotros ya sabemos trabajar con esta librer칤a no deber칤amos tener muchos problemas para aprender a trabajar con `Pytorch` 游때."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "De la misma manera que en `NumPy` el objeto principal es el `ndarray`, en `Pytorch` el objeto principal es el `tensor`. Podemos definir un tensor de manera similar a como definimos un array, incluso podemos inicializar tensores a partir de arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:10.873767Z",
     "start_time": "2020-08-15T09:29:10.860741Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matriz de ceros, 5 filas y 3 columnas\n",
    "\n",
    "x = torch.zeros(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:37.744086Z",
     "start_time": "2020-08-15T09:30:37.715079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4085,  0.1492],\n",
       "         [-0.3065, -0.3972],\n",
       "         [-0.9104,  0.4946]],\n",
       "\n",
       "        [[-1.0368, -1.8317],\n",
       "         [-0.2983,  0.1285],\n",
       "         [-0.2682, -1.6739]],\n",
       "\n",
       "        [[ 0.1817, -0.4391],\n",
       "         [-1.5879,  0.4627],\n",
       "         [ 0.4037,  0.2123]],\n",
       "\n",
       "        [[ 0.8303,  0.2736],\n",
       "         [-1.5003,  0.7714],\n",
       "         [-0.8558,  0.1547]],\n",
       "\n",
       "        [[ 1.2789,  1.3012],\n",
       "         [ 0.3494,  2.0338],\n",
       "         [-1.3126, -0.1815]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor con valores aleatorios\n",
    "\n",
    "x = torch.randn(5, 3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:41.310781Z",
     "start_time": "2020-08-15T09:29:41.290558Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor a partir de lista \n",
    "\n",
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:05.108750Z",
     "start_time": "2020-08-15T09:30:05.090922Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tensor a partir de array\n",
    "\n",
    "a = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "x = torch.from_numpy(a)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y como puedes esperar, pr치cticamente todos los conceptos que ya conocemos para trabajar con `NumPy` pueden aplicarse en `Pytorch`. Esto incluye operaciones aritm칠ticas, indexado y troceado, iteraci칩n, vectorizaci칩n y broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:32.797001Z",
     "start_time": "2020-08-15T09:33:32.788000Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6251,  0.2682, -0.0485],\n",
       "         [ 1.7609,  0.0925,  0.1266],\n",
       "         [ 0.3453, -1.6688,  0.4845]]),\n",
       " tensor([[-1.0122, -0.7118, -0.2288],\n",
       "         [ 2.0132, -1.6643,  0.8877],\n",
       "         [ 0.3299,  0.9017,  0.1258]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operaciones\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:36.035260Z",
     "start_time": "2020-08-15T09:33:36.017231Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3871, -0.4436, -0.2773],\n",
       "        [ 3.7740, -1.5718,  1.0144],\n",
       "        [ 0.6752, -0.7671,  0.6103]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:39.567010Z",
     "start_time": "2020-08-15T09:33:39.553201Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6373,  0.9800,  0.1804],\n",
       "        [-0.2523,  1.7568, -0.7611],\n",
       "        [ 0.0154, -2.5705,  0.3587]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:28.502097Z",
     "start_time": "2020-08-15T09:34:28.490089Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6251,  0.2682, -0.0485])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexado\n",
    "\n",
    "# primera fila\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:36.631008Z",
     "start_time": "2020-08-15T09:34:36.625987Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6251)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera fila, primera columna\n",
    "\n",
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:44.819616Z",
     "start_time": "2020-08-15T09:34:44.811625Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6251,  0.2682, -0.0485])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera columna\n",
    "\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:35:30.156589Z",
     "start_time": "2020-08-15T09:35:30.133710Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2682, -0.0485],\n",
       "        [ 0.0925,  0.1266]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troceado\n",
    "\n",
    "x[:-1, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Una funcionalidad importante del objeto `tensor` que utilizaremos muy a menudo es cambiar su forma. Esto lo conseguimos con la funci칩n `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:12.220185Z",
     "start_time": "2020-08-15T09:37:12.209120Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:52.119999Z",
     "start_time": "2020-08-15T09:37:52.100991Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a침adimos una dimensi칩n extra\n",
    "\n",
    "x.view(1, 3, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:38:04.360901Z",
     "start_time": "2020-08-15T09:38:04.349867Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estiramos en una sola dimensi칩n\n",
    "\n",
    "x.view(9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:15.139898Z",
     "start_time": "2020-08-15T09:39:15.130902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usamos -1 para asignar todos los valores restantes a una dimensi칩n\n",
    "\n",
    "x.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Podemos transformar un `tensor` en un `array` con la funci칩n `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:46.534626Z",
     "start_time": "2020-08-15T09:39:46.515264Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6251061 ,  0.26821104, -0.04845389],\n",
       "       [ 1.7608535 ,  0.09249022,  0.12663704],\n",
       "       [ 0.34528732, -1.6688324 ,  0.4844624 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, un `tensor` de `Pytorch` es muy similar a un `array` de `NumPy`. Aqu칤 hemos visto alguna de la funcionalidad m치s 칰til, puedes aprender m치s [aqu칤](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ya hemos visto que `Pytorch` es muy similar a `NumPy`, sin embargo su funcionalidad va m치s all치 de una estructura de datos eficiente con la que podemos llevar a cabo operaciones (para eso ya nos basta con `NumPy`). La funcionalidad m치s importante que `Pytorch` a침ade es la conocidad como `autograd`, la cual nos proporciona la posibilidad de calcular derivadas de manera autom치tica con respecto a cualquier `tensor`. Esto le da a `Pytorch` un gran potencial para dise침ar `redes neuronales` complejas y entrenarlas utilizando algoritmos de gradientes sin tener que calcular todas estas derivadas manualmente (como hemos hecho en los posts anteriores). Para poder llevar a cabo estas operaciones, `Pytorch` va construyendo de manera din치mica un `grafo computacional`. Cada vez que aplicamos una operaci칩n sobre uno o varios tensores, 칠stos se a침aden al `grafo computacional` junto a la operaci칩n en concreto. De esta manera, si queremos calcular la derivada de cualquier valor con respecto a cualquier tensor, simplemente tenemos que aplicar el algoritmo de `backpropagation` (que no es m치s que la regla de la cadena de la derivada) en el `grafo`. Vamos a ilustrarlo con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:47.830227Z",
     "start_time": "2020-08-15T09:52:47.810220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(2., requires_grad=True)\n",
    "p = x + y\n",
    "\n",
    "z = torch.tensor(3., requires_grad=True)\n",
    "g = p * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En la celda anterior hemos definido tres `tensores`: $x$, $y$ y $z$. En primer lugar, para poder calcular derivadas con respecto a estos tensores necesitamos ponder su propiedad `requiers_grad` a `True`. Ahora, calculamos el tensor intermedio $p$ como $p = x+ y$ y luego usamos este valor para calcular el resultado final $g$ como $g = p*z$. Cada vez que aplicamos una operaci칩n sobre un tensor que tiene su propiedad `requires_grad` a `True`, `Pytorch` ir치 construyendo el `grafo computacional`. Para este ejemplo, el grafo tendr칤a la siguiente forma\n",
    "\n",
    "![](https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg)\n",
    "\n",
    "Si ahora queremos calcular las derivadas de $g$ con respecto a $x$, $y$ y $z$, es tan f치cil como llamar a la funci칩n `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:48.341446Z",
     "start_time": "2020-08-15T09:52:48.335900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En este punto, `Pytorch` ha aplicado el algoritmo de `backpropagation` encima del grafo computacional, calculando todas las derivadas.\n",
    "\n",
    "$$ \\frac{dg}{dz} = p $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:55:50.796060Z",
     "start_time": "2020-08-15T09:55:50.789058Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\frac{dp}{dx} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:32.429083Z",
     "start_time": "2020-08-15T09:56:32.423080Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\frac{dp}{dy} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:47.233861Z",
     "start_time": "2020-08-15T09:56:47.214278Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, el `grafo computacional` es una herramienta extraordinaria para dise침ar `redes neuronales` de complejidad arbitraria. Con una simple funci칩n, gracias al algoritmo de `backpropagation`, podemos calcular todas las derivadas de manera sencilla (cada nodo que representa una operaci칩n solo necesita calcular su propia derivada de manera local) y optimizar el modelo con nuestro algoritmo de gradiente preferido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> 游눠 Sabiendo que el `perceptr칩n` lleva a cabo la operaci칩n $\\hat{y} = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$, 쯦e ves capaz de dibujar su grafo computacional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A침adiendo `autograd` encima de `NumPy`, `Pytorch` nos ofrece todo lo que necesitamos para dise침ar y entrenar `redes neuronales`. Puedes aprender m치s sobre `autograd` [aqu칤](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py). Sin embargo, si queremos entrenar redes muy grandes o utilizar datasets muy grandes (o ambas), el proceso de entrenamiento ser치 muy lento. Es aqu칤 donde entra en juego el 칰ltimo elemento que hace de `Pytorch` lo que es. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:19:44.098334Z",
     "start_time": "2020-08-15T10:19:44.084325Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobar que podemos usar GPU\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:06.036817Z",
     "start_time": "2020-08-15T10:21:04.774034Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 283 ms, total: 488 ms\n",
      "Wall time: 48.3 ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000,10000)\n",
    "y = torch.randn(10000,10000)\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:08.211624Z",
     "start_time": "2020-08-15T10:21:06.778483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(10000,10000).cuda()\n",
    "y = torch.randn(10000,10000).cuda()\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, llevar a cabo operaciones con grandes tensores en una GPU en vez de la CPU puede resultar en una considerable reducci칩n del tiempo de c치lculo. Todas las siguientes maneras son v치lidas para copiar un tensor en una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T11:05:29.078172Z",
     "start_time": "2020-08-15T11:05:29.064175Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-fbab5100b4f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.randn((10000,10000), device=\"cuda\")\n",
    "x = x.cuda()\n",
    "x = x.to(\"cuda\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y para volver a copiar un `tensor` de vuelta en la CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:23:37.518832Z",
     "start_time": "2020-08-15T10:23:37.359224Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = x.cpu()\n",
    "x = x.to(\"cpu\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, simplemente definiendo los `tensores` para los pesos y los datos y copi치ndolos a la GPU podemos definir el `grafo computacional` de manera din치mica aplicando operaciones sobre los tensores (multiplicamos por los pesos y sumamos el *bias*). Una vez tenemos la salida del `MLP` calculamos la funci칩n de p칠rdida y llamando a la funci칩n `backward` `Pytorch` se encarga de calcular todas las derivadas de manera autom치tica. Una vez tenemos los gradientes con respecto a los pesos, podemos actualizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto una introducci칩n a `Pytorch`, un framework de `redes neuronales` muy utilizado a d칤a de hoy. Hemos visto que `Pytorch` es muy similar a `NumPy` y comparten gran parte de su sintaxis, lo cual es una ventaja si ya sabemos trabajar con `NumPy`. Adem치s, a침ade `autograd`, la capacidad de construir de manera din치mica un `grafo computacional` de manera que en cualquier momento podemos calcular derivadas con respecto a cualquier tensor de manera autom치tica. Por 칰ltimo, hemos visto como podemos ejecutar todas estas operaciones en una GPU para acelerar el proceso de entrenamiento de nuestros modelos de `Deep Learning`. Este es el n칰cleo de `Pytorch`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
