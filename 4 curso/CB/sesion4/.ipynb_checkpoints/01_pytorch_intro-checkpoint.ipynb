{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Introducci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:21:50.890591Z",
     "start_time": "2020-08-15T09:21:49.757500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬ø Qu√© es Pytorch ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` es un framework de `redes neuronales`, un conjunto de librer√≠as y herramientas que nos hacen la vida m√°s f√°cil a la hora de dise√±ar, entrenar y poner en producci√≥n nuestros modelos de `Deep Learning`. Una forma sencilla de entender qu√© es `Pytorch` es la siguiente:\n",
    "\n",
    "$$ Pytorch = Numpy + Autograd + GPU $$\n",
    "\n",
    "Vamos a ver qu√© significa cada uno de estos t√©rminos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Quiz√°s la caracter√≠stica m√°s relevante de `Pytorch` es su facilidad de uso. Esto es debido a que sigue una interfaz muy similar a la de `NumPy`, y como nosotros ya sabemos trabajar con esta librer√≠a no deber√≠amos tener muchos problemas para aprender a trabajar con `Pytorch` üòÅ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "De la misma manera que en `NumPy` el objeto principal es el `ndarray`, en `Pytorch` el objeto principal es el `tensor`. Podemos definir un tensor de manera similar a como definimos un array, incluso podemos inicializar tensores a partir de arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:10.873767Z",
     "start_time": "2020-08-15T09:29:10.860741Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matriz de ceros, 5 filas y 3 columnas\n",
    "\n",
    "x = torch.zeros(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:37.744086Z",
     "start_time": "2020-08-15T09:30:37.715079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6858, -1.0103],\n",
       "         [-0.9205,  1.1782],\n",
       "         [-0.0633, -0.5956]],\n",
       "\n",
       "        [[-1.2671,  0.7387],\n",
       "         [ 0.3122, -0.1043],\n",
       "         [-1.6743, -0.4204]],\n",
       "\n",
       "        [[ 0.5346, -2.9931],\n",
       "         [ 0.4544, -0.5907],\n",
       "         [-1.1219,  0.4068]],\n",
       "\n",
       "        [[-0.4841,  1.2511],\n",
       "         [-1.1112, -0.8510],\n",
       "         [ 1.2044,  0.5798]],\n",
       "\n",
       "        [[ 0.1264, -0.6565],\n",
       "         [ 0.2967,  0.2706],\n",
       "         [-0.9368,  0.4513]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor con valores aleatorios\n",
    "\n",
    "x = torch.randn(5, 3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:29:41.310781Z",
     "start_time": "2020-08-15T09:29:41.290558Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor a partir de lista \n",
    "\n",
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:30:05.108750Z",
     "start_time": "2020-08-15T09:30:05.090922Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tensor a partir de array\n",
    "\n",
    "a = np.array([[1, 2, 3],[4, 5, 6]])\n",
    "x = torch.from_numpy(a)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y como puedes esperar, pr√°cticamente todos los conceptos que ya conocemos para trabajar con `NumPy` pueden aplicarse en `Pytorch`. Esto incluye operaciones aritm√©ticas, indexado y troceado, iteraci√≥n, vectorizaci√≥n y broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:32.797001Z",
     "start_time": "2020-08-15T09:33:32.788000Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4589, -0.5650,  1.3569],\n",
       "         [-0.8769,  0.0908, -0.0896],\n",
       "         [ 1.1308,  1.9795,  0.3431]]),\n",
       " tensor([[ 0.4883,  1.0040, -1.2333],\n",
       "         [-0.8284, -0.4846,  0.4495],\n",
       "         [-0.5211,  0.1418,  0.3862]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operaciones\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:36.035260Z",
     "start_time": "2020-08-15T09:33:36.017231Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0294,  0.4389,  0.1236],\n",
       "        [-1.7053, -0.3938,  0.3599],\n",
       "        [ 0.6097,  2.1213,  0.7293]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:33:39.567010Z",
     "start_time": "2020-08-15T09:33:39.553201Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9471, -1.5690,  2.5902],\n",
       "        [-0.0485,  0.5755, -0.5391],\n",
       "        [ 1.6519,  1.8376, -0.0432]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:28.502097Z",
     "start_time": "2020-08-15T09:34:28.490089Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4589, -0.5650,  1.3569])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexado\n",
    "\n",
    "# primera fila\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:36.631008Z",
     "start_time": "2020-08-15T09:34:36.625987Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4589)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera fila, primera columna\n",
    "\n",
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:34:44.819616Z",
     "start_time": "2020-08-15T09:34:44.811625Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4589, -0.5650,  1.3569])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera columna\n",
    "\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:35:30.156589Z",
     "start_time": "2020-08-15T09:35:30.133710Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5650,  1.3569],\n",
       "        [ 0.0908, -0.0896]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troceado\n",
    "\n",
    "x[:-1, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Una funcionalidad importante del objeto `tensor` que utilizaremos muy a menudo es cambiar su forma. Esto lo conseguimos con la funci√≥n `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:12.220185Z",
     "start_time": "2020-08-15T09:37:12.209120Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:37:52.119999Z",
     "start_time": "2020-08-15T09:37:52.100991Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a√±adimos una dimensi√≥n extra\n",
    "\n",
    "x.view(1, 3, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:38:04.360901Z",
     "start_time": "2020-08-15T09:38:04.349867Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estiramos en una sola dimensi√≥n\n",
    "\n",
    "x.view(9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:15.139898Z",
     "start_time": "2020-08-15T09:39:15.130902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usamos -1 para asignar todos los valores restantes a una dimensi√≥n\n",
    "\n",
    "x.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Podemos transformar un `tensor` en un `array` con la funci√≥n `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:39:46.534626Z",
     "start_time": "2020-08-15T09:39:46.515264Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45886418, -0.5650464 ,  1.3568938 ],\n",
       "       [-0.8768579 ,  0.09084709, -0.08956412],\n",
       "       [ 1.1307832 ,  1.979452  ,  0.34309483]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, un `tensor` de `Pytorch` es muy similar a un `array` de `NumPy`. Aqu√≠ hemos visto alguna de la funcionalidad m√°s √∫til, puedes aprender m√°s [aqu√≠](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ya hemos visto que `Pytorch` es muy similar a `NumPy`, sin embargo su funcionalidad va m√°s all√° de una estructura de datos eficiente con la que podemos llevar a cabo operaciones (para eso ya nos basta con `NumPy`). La funcionalidad m√°s importante que `Pytorch` a√±ade es la conocidad como `autograd`, la cual nos proporciona la posibilidad de calcular derivadas de manera autom√°tica con respecto a cualquier `tensor`. Esto le da a `Pytorch` un gran potencial para dise√±ar `redes neuronales` complejas y entrenarlas utilizando algoritmos de gradientes sin tener que calcular todas estas derivadas manualmente (como hemos hecho en los posts anteriores). Para poder llevar a cabo estas operaciones, `Pytorch` va construyendo de manera din√°mica un `grafo computacional`. Cada vez que aplicamos una operaci√≥n sobre uno o varios tensores, √©stos se a√±aden al `grafo computacional` junto a la operaci√≥n en concreto. De esta manera, si queremos calcular la derivada de cualquier valor con respecto a cualquier tensor, simplemente tenemos que aplicar el algoritmo de `backpropagation` (que no es m√°s que la regla de la cadena de la derivada) en el `grafo`. Vamos a ilustrarlo con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:47.830227Z",
     "start_time": "2020-08-15T09:52:47.810220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(2., requires_grad=True)\n",
    "p = x + y\n",
    "\n",
    "z = torch.tensor(3., requires_grad=True)\n",
    "g = p * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En la celda anterior hemos definido tres `tensores`: $x$, $y$ y $z$. En primer lugar, para poder calcular derivadas con respecto a estos tensores necesitamos ponder su propiedad `requiers_grad` a `True`. Ahora, calculamos el tensor intermedio $p$ como $p = x+ y$ y luego usamos este valor para calcular el resultado final $g$ como $g = p*z$. Cada vez que aplicamos una operaci√≥n sobre un tensor que tiene su propiedad `requires_grad` a `True`, `Pytorch` ir√° construyendo el `grafo computacional`. Para este ejemplo, el grafo tendr√≠a la siguiente forma\n",
    "\n",
    "![](https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg)\n",
    "\n",
    "Si ahora queremos calcular las derivadas de $g$ con respecto a $x$, $y$ y $z$, es tan f√°cil como llamar a la funci√≥n `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:52:48.341446Z",
     "start_time": "2020-08-15T09:52:48.335900Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En este punto, `Pytorch` ha aplicado el algoritmo de `backpropagation` encima del grafo computacional, calculando todas las derivadas.\n",
    "\n",
    "$$ \\frac{dg}{dz} = p $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:55:50.796060Z",
     "start_time": "2020-08-15T09:55:50.789058Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\frac{dp}{dx} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:32.429083Z",
     "start_time": "2020-08-15T09:56:32.423080Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\frac{dp}{dy} = z $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:56:47.233861Z",
     "start_time": "2020-08-15T09:56:47.214278Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes ver, el `grafo computacional` es una herramienta extraordinaria para dise√±ar `redes neuronales` de complejidad arbitraria. Con una simple funci√≥n, gracias al algoritmo de `backpropagation`, podemos calcular todas las derivadas de manera sencilla (cada nodo que representa una operaci√≥n solo necesita calcular su propia derivada de manera local) y optimizar el modelo con nuestro algoritmo de gradiente preferido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> üí° Sabiendo que el `perceptr√≥n` lleva a cabo la operaci√≥n $\\hat{y} = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$, ¬øte ves capaz de dibujar su grafo computacional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A√±adiendo `autograd` encima de `NumPy`, `Pytorch` nos ofrece todo lo que necesitamos para dise√±ar y entrenar `redes neuronales`. Puedes aprender m√°s sobre `autograd` [aqu√≠](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py). Sin embargo, si queremos entrenar redes muy grandes o utilizar datasets muy grandes (o ambas), el proceso de entrenamiento ser√° muy lento. Es aqu√≠ donde entra en juego el √∫ltimo elemento que hace de `Pytorch` lo que es. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:19:44.098334Z",
     "start_time": "2020-08-15T10:19:44.084325Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobar que podemos usar GPU\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:06.036817Z",
     "start_time": "2020-08-15T10:21:04.774034Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 283 ms, total: 488 ms\n",
      "Wall time: 48.3 ms\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000,10000)\n",
    "y = torch.randn(10000,10000)\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:21:08.211624Z",
     "start_time": "2020-08-15T10:21:06.778483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(10000,10000).cuda()\n",
    "y = torch.randn(10000,10000).cuda()\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, llevar a cabo operaciones con grandes tensores en una GPU en vez de la CPU puede resultar en una considerable reducci√≥n del tiempo de c√°lculo. Todas las siguientes maneras son v√°lidas para copiar un tensor en una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T11:05:29.078172Z",
     "start_time": "2020-08-15T11:05:29.064175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.randn((10000,10000), device=\"cuda\")\n",
    "x = x.cuda()\n",
    "x = x.to(\"cuda\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Y para volver a copiar un `tensor` de vuelta en la CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T10:23:37.518832Z",
     "start_time": "2020-08-15T10:23:37.359224Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = x.cpu()\n",
    "x = x.to(\"cpu\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar, simplemente definiendo los `tensores` para los pesos y los datos y copi√°ndolos a la GPU podemos definir el `grafo computacional` de manera din√°mica aplicando operaciones sobre los tensores (multiplicamos por los pesos y sumamos el *bias*). Una vez tenemos la salida del `MLP` calculamos la funci√≥n de p√©rdida y llamando a la funci√≥n `backward` `Pytorch` se encarga de calcular todas las derivadas de manera autom√°tica. Una vez tenemos los gradientes con respecto a los pesos, podemos actualizarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto una introducci√≥n a `Pytorch`, un framework de `redes neuronales` muy utilizado a d√≠a de hoy. Hemos visto que `Pytorch` es muy similar a `NumPy` y comparten gran parte de su sintaxis, lo cual es una ventaja si ya sabemos trabajar con `NumPy`. Adem√°s, a√±ade `autograd`, la capacidad de construir de manera din√°mica un `grafo computacional` de manera que en cualquier momento podemos calcular derivadas con respecto a cualquier tensor de manera autom√°tica. Por √∫ltimo, hemos visto como podemos ejecutar todas estas operaciones en una GPU para acelerar el proceso de entrenamiento de nuestros modelos de `Deep Learning`. Este es el n√∫cleo de `Pytorch`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
